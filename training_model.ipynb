{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from torch.nn import init\n",
    "import copy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RND(nn.Module):\n",
    "    def __init__(self,width = 240, height =256, channel = 3):\n",
    "        super(RND,self).__init__()\n",
    "        \n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.channel = channel\n",
    "        \n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=3,\n",
    "                out_channels=32,\n",
    "                kernel_size=8,\n",
    "                stride=4),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(\n",
    "                in_channels=32,\n",
    "                out_channels=64,\n",
    "                kernel_size=4,\n",
    "                stride=2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(\n",
    "                in_channels=64,\n",
    "                out_channels=64,\n",
    "                kernel_size=3,\n",
    "                stride=1),\n",
    "            nn.LeakyReLU(),\n",
    "            Flatten(),\n",
    "            nn.Linear(46592, 512), # change\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512)\n",
    "            )\n",
    "        self.target = copy.deepcopy(self.predictor)\n",
    "        for p in self.modules():\n",
    "            if isinstance(p, nn.Conv2d):\n",
    "                init.orthogonal_(p.weight, np.sqrt(2))\n",
    "                p.bias.data.zero_()\n",
    "\n",
    "            if isinstance(p, nn.Linear):\n",
    "                init.orthogonal_(p.weight, np.sqrt(2))\n",
    "                p.bias.data.zero_()\n",
    "\n",
    "        for param in self.target.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "    def forward(self, state):\n",
    "        target_feature = self.target(state)\n",
    "        predict_feature = self.predictor(state)\n",
    "        return predict_feature, target_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "class PPO(nn.Module):\n",
    "    def __init__(self,width = 240, height =256, channel = 3, action_dim = 7):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.channel = channel\n",
    "        self.action_dim = action_dim\n",
    "        super(PPO,self).__init__()\n",
    "        self.memory = []\n",
    "        \n",
    "        self.basic = nn.Sequential(\\\n",
    "                    nn.Conv2d(in_channels = 3,\\\n",
    "                             out_channels = 32,\\\n",
    "                             kernel_size = 8,\\\n",
    "                              stride = 4),\n",
    "                    nn.ReLU(),\n",
    "                                   nn.Conv2d(in_channels = 32,\\\n",
    "                                           out_channels = 64,\\\n",
    "                                           kernel_size = 4,\\\n",
    "                                           stride = 2),\\\n",
    "                    nn.ReLU(),\\\n",
    "                    nn.Conv2d(in_channels = 64,\\\n",
    "                             out_channels = 64,\\\n",
    "                             kernel_size = 3,\\\n",
    "                             stride = 1),\n",
    "                                   nn.ReLU(),\\\n",
    "                                   Flatten(),\n",
    "                    nn.Linear(46592,256), #have to change       \n",
    "                    nn.ReLU(),\\\n",
    "                    nn.Linear(256,448),\n",
    "                    nn.ReLU()\n",
    "                    )\n",
    "        self.actor = nn.Sequential(\\\n",
    "                                  nn.Linear(448,448),\\\n",
    "                                  nn.ReLU(),\\\n",
    "                                  nn.Linear(448,self.action_dim)\\\n",
    "                                  )\n",
    "        \n",
    "        self.extrinsic_critic = nn.Linear(448,1)\n",
    "        self.intrinsic_critic = nn.Linear(448,1)\n",
    "        \n",
    "        init.orthogonal_(self.extrinsic_critic.weight, 0.01)\n",
    "        self.extrinsic_critic.bias.data.zero_()\n",
    "\n",
    "        init.orthogonal_(self.intrinsic_critic.weight, 0.01)\n",
    "        self.intrinsic_critic.bias.data.zero_()\n",
    "\n",
    "        for i in range(len(self.actor)):\n",
    "            if type(self.actor[i]) == nn.Linear:\n",
    "                init.orthogonal_(self.actor[i].weight, 0.01)\n",
    "                self.actor[i].bias.data.zero_()\n",
    "    def forward(self, x,dim = -1):\n",
    "        x = self.basic(x)\n",
    "        action = self.actor(x)\n",
    "        action_prob = F.softmax(action,dim = dim)\n",
    "        \n",
    "        intrinsic = self.intrinsic_critic(x)\n",
    "        extrinsic = self.extrinsic_critic(x)\n",
    "        return action_prob,extrinsic,intrinsic    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "gamma         = 0.98\n",
    "lmbda         = 0.95\n",
    "eps_clip      = 0.1\n",
    "K_epoch       = 4\n",
    "T_horizon     = 128\n",
    "critic_coef = 0.5\n",
    "ent_coef = 0.001\n",
    "update_proportion = 0.25\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self,width=240,height=256,channel = 3,action_dim=7,learning_rate=0.0005):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.channel = channel\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        super(Agent,self).__init__()\n",
    "        \n",
    "        self.memory = []\n",
    "        \n",
    "        self.ppo = PPO(self.width, self.height, self.channel, self.action_dim)\n",
    "        self.rnd = RND(self.width, self.height , self.channel)\n",
    "        self.optimizer = optim.Adam(self.parameters(),lr = learning_rate)\n",
    "    def put_data(self,data):\n",
    "        self.memory.append(data)\n",
    "    def make_batch(self):\n",
    "        state_list, action_list, extrinsic_reward_list, intrinsic_reward_list, next_state_list, \\\n",
    "        prob_list, extrinsic_done_list,intrinsic_done_list = [],[],[],[],[],[], [],[]\n",
    "        for data in self.memory:\n",
    "            state,action,extrinsic_reward, intrinsic_reward,next_state,prob,done = data\n",
    "            state_list.append(state)\n",
    "            action_list.append([action])\n",
    "            extrinsic_reward_list.append([extrinsic_reward])\n",
    "            intrinsic_reward_list.append([intrinsic_reward])\n",
    "\n",
    "            next_state_list.append(next_state)\n",
    "            extrinsic_done_mask = 0 if done else 1\n",
    "            extrinsic_done_list.append([extrinsic_done_mask])\n",
    "            intrinsic_done_list.append([1])\n",
    "            prob_list.append([prob])\n",
    "        self.memory = []\n",
    "\n",
    "        s,a,er,ir,next_s,extrinsic_done_list,intrinsic_done_list,prob \\\n",
    "                                        = torch.tensor(state_list,dtype=torch.float),\\\n",
    "                                        torch.tensor(action_list),\\\n",
    "                                        torch.tensor(extrinsic_reward_list,dtype = torch.float),\\\n",
    "                                        torch.tensor(intrinsic_reward_list,dtype = torch.float),\\\n",
    "                                        torch.tensor(next_state_list,dtype=torch.float),\\\n",
    "                                        torch.tensor(extrinsic_done_list,dtype = torch.float),\\\n",
    "                                        torch.tensor(intrinsic_done_list,dtype = torch.float),\\\n",
    "                                        torch.tensor(prob_list,dtype = torch.float)\n",
    "        if gpu:\n",
    "            return s.cuda(),a.cuda(),er.cuda(),ir.cuda(),next_s.cuda(),extrinsic_done_list.cuda()\\\n",
    "            ,intrinsic_done_list.cuda(),prob.cuda() \n",
    "        else :\n",
    "            return s,a,er,ir,next_s,extrinsic_done_list,intrinsic_done_list,prob  \n",
    "    \n",
    "    def train(self):\n",
    "        state,action,extrinsic_reward,intrinsic_reward, next_state,extrinsic_done_list,\\\n",
    "        intrinsic_done_list,action_prob = self.make_batch()\n",
    "        \n",
    "        for i in range(K_epoch):\n",
    "            state = state.squeeze()\n",
    "            next_state = next_state.squeeze()\n",
    "            predicted_action, predicted_extrinsic, predicted_intrinsic = self.ppo(state)\n",
    "            predicted_next_action, predicted_next_extrinsic, predicted_next_intrinsic = self.ppo(next_state)\n",
    "            predict_feature, target_feature = self.rnd(next_state)\n",
    "            td_error = extrinsic_reward + gamma * predicted_next_extrinsic * extrinsic_done_list\n",
    "            delta = td_error - predicted_next_extrinsic\n",
    "            if gpu:\n",
    "                delta = delta.detach().cpu().numpy()\n",
    "            else:\n",
    "                delta = delta.detach().numpy()\n",
    "            advantage_list = []\n",
    "            \n",
    "            advantage = 0.0\n",
    "            for delta_t in delta[::-1]:\n",
    "                advantage = gamma * lmbda * advantage + delta_t[0]\n",
    "                advantage_list.append([advantage])\n",
    "            advantage_list.reverse()\n",
    "            if gpu:\n",
    "                advantage = torch.tensor(advantage_list,dtype = torch.float).cuda()\n",
    "            else:\n",
    "                advantage = torch.tensor(advantage_list,dtype = torch.float)\n",
    "            \n",
    "            intrinsic_td_error = intrinsic_reward + gamma * predicted_next_intrinsic * intrinsic_done_list\n",
    "            \n",
    "            intrinsic_error = (intrinsic_td_error - predicted_intrinsic.detach()).pow(2)\n",
    "            if gpu:\n",
    "                masking = torch.rand(len(intrinsic_error)).cuda()\n",
    "                masking = (masking < update_proportion).type(torch.FloatTensor).cuda()\n",
    "            else:\n",
    "                masking = torch.rand(len(intrinsic_error))\n",
    "                masking = (masking < update_proportion).type(torch.FloatTensor)\n",
    "            if gpu:\n",
    "                intrinsic_error = (intrinsic_error * masking).sum() / torch.max(intrinsic_error.sum(), torch.Tensor([1]).cuda())\n",
    "            else:\n",
    "                intrinsic_error = (intrinsic_error * masking).sum() / torch.max(masking.sum(), torch.Tensor([1]))\n",
    "\n",
    "            \n",
    "            now_action = predicted_action\n",
    "            m = Categorical(now_action)\n",
    "            entropy = m.entropy().mean()\n",
    "            \n",
    "            \n",
    "            now_action = now_action.gather(1,action)\n",
    "            \n",
    "            \n",
    "            ratio = torch.exp(torch.log(now_action) - torch.log(action_prob))\n",
    "            \n",
    "            surr1 = ratio * advantage\n",
    "            surr2 = torch.clamp(ratio , 1-eps_clip, 1 + eps_clip) * advantage\n",
    "            loss = - torch.min(surr1,surr2) + critic_coef * (F.smooth_l1_loss(predicted_extrinsic,td_error.detach()) +\\\n",
    "                    intrinsic_error) + ent_coef * entropy + F.mse_loss(predict_feature, target_feature)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
    "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if gpu:\n",
    "    model = Agent().cuda()\n",
    "else:\n",
    "    model = Agent()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['training_model.ipynb', 'weights']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob.glob(\"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "if gpu:\n",
    "    model.load_state_dict(torch.load(\"weights/3_epochs_model\"))\n",
    "else:\n",
    "    model.load_state_dict(torch.load(\"weights/3_epochs_model\", map_location={'cuda:0': 'cpu'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step :  1 , action :  0 action_prob :  [0.14285683631896973, 0.14285725355148315, 0.14285720884799957, 0.14285777509212494, 0.14285695552825928, 0.14285697042942047, 0.14285700023174286]\n",
      "extrinsic_reward :  0 intrinsic_reward :  410.8612060546875\n",
      "place 40\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 40, 'x_pos_screen': 40, 'y_pos': 79}\n",
      "global_step :  2 , action :  1 action_prob :  [0.14285683631896973, 0.14285725355148315, 0.14285720884799957, 0.14285777509212494, 0.14285695552825928, 0.14285697042942047, 0.14285700023174286]\n",
      "extrinsic_reward :  0 intrinsic_reward :  410.8612060546875\n",
      "place 40\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 40, 'x_pos_screen': 40, 'y_pos': 79}\n",
      "global_step :  3 , action :  3 action_prob :  [0.14285683631896973, 0.14285725355148315, 0.14285720884799957, 0.14285777509212494, 0.14285695552825928, 0.14285697042942047, 0.14285700023174286]\n",
      "extrinsic_reward :  0 intrinsic_reward :  410.8612060546875\n",
      "place 40\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 40, 'x_pos_screen': 40, 'y_pos': 79}\n",
      "global_step :  4 , action :  5 action_prob :  [0.14285683631896973, 0.14285725355148315, 0.14285720884799957, 0.14285777509212494, 0.14285695552825928, 0.14285697042942047, 0.14285700023174286]\n",
      "extrinsic_reward :  0 intrinsic_reward :  410.30364990234375\n",
      "place 40\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 40, 'x_pos_screen': 40, 'y_pos': 79}\n",
      "global_step :  5 , action :  0 action_prob :  [0.14285683631896973, 0.14285725355148315, 0.14285720884799957, 0.14285777509212494, 0.14285695552825928, 0.14285697042942047, 0.14285700023174286]\n",
      "extrinsic_reward :  0 intrinsic_reward :  410.9037170410156\n",
      "place 40\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 40, 'x_pos_screen': 40, 'y_pos': 84}\n",
      "global_step :  6 , action :  3 action_prob :  [0.14285683631896973, 0.14285725355148315, 0.14285722374916077, 0.14285777509212494, 0.14285695552825928, 0.14285697042942047, 0.14285700023174286]\n",
      "extrinsic_reward :  0 intrinsic_reward :  412.1300048828125\n",
      "place 40\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 40, 'x_pos_screen': 40, 'y_pos': 89}\n",
      "global_step :  7 , action :  5 action_prob :  [0.14285683631896973, 0.14285725355148315, 0.14285722374916077, 0.14285777509212494, 0.14285695552825928, 0.14285697042942047, 0.14285701513290405]\n",
      "extrinsic_reward :  0 intrinsic_reward :  414.16632080078125\n",
      "place 40\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 40, 'x_pos_screen': 40, 'y_pos': 93}\n",
      "global_step :  8 , action :  5 action_prob :  [0.14285683631896973, 0.14285725355148315, 0.14285722374916077, 0.14285777509212494, 0.14285695552825928, 0.14285697042942047, 0.14285700023174286]\n",
      "extrinsic_reward :  1 intrinsic_reward :  412.11016845703125\n",
      "place 41\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 41, 'x_pos_screen': 41, 'y_pos': 96}\n",
      "global_step :  9 , action :  2 action_prob :  [0.14285683631896973, 0.14285725355148315, 0.14285720884799957, 0.14285777509212494, 0.14285695552825928, 0.14285697042942047, 0.14285700023174286]\n",
      "extrinsic_reward :  0 intrinsic_reward :  412.45452880859375\n",
      "place 41\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 41, 'x_pos_screen': 41, 'y_pos': 99}\n",
      "global_step :  10 , action :  0 action_prob :  [0.14285683631896973, 0.14285725355148315, 0.14285720884799957, 0.14285777509212494, 0.14285695552825928, 0.14285697042942047, 0.14285700023174286]\n",
      "extrinsic_reward :  0 intrinsic_reward :  413.34063720703125\n",
      "place 41\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 41, 'x_pos_screen': 41, 'y_pos': 101}\n",
      "global_step :  11 , action :  3 action_prob :  [0.14285685122013092, 0.14285726845264435, 0.14285722374916077, 0.14285778999328613, 0.14285697042942047, 0.14285698533058167, 0.14285700023174286]\n",
      "extrinsic_reward :  1 intrinsic_reward :  411.8522644042969\n",
      "place 42\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 42, 'x_pos_screen': 42, 'y_pos': 102}\n",
      "global_step :  12 , action :  6 action_prob :  [0.14285685122013092, 0.14285726845264435, 0.14285722374916077, 0.14285778999328613, 0.14285697042942047, 0.14285698533058167, 0.14285700023174286]\n",
      "extrinsic_reward :  0 intrinsic_reward :  412.8841552734375\n",
      "place 42\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 42, 'x_pos_screen': 42, 'y_pos': 103}\n",
      "global_step :  13 , action :  6 action_prob :  [0.14285683631896973, 0.14285725355148315, 0.14285722374916077, 0.14285777509212494, 0.14285695552825928, 0.14285697042942047, 0.14285700023174286]\n",
      "extrinsic_reward :  0 intrinsic_reward :  413.78631591796875\n",
      "place 42\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 42, 'x_pos_screen': 42, 'y_pos': 103}\n",
      "global_step :  14 , action :  0 action_prob :  [0.14285685122013092, 0.14285726845264435, 0.14285723865032196, 0.14285778999328613, 0.14285697042942047, 0.14285698533058167, 0.14285700023174286]\n",
      "extrinsic_reward :  1 intrinsic_reward :  413.78631591796875\n",
      "place 43\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 43, 'x_pos_screen': 43, 'y_pos': 102}\n",
      "global_step :  15 , action :  2 action_prob :  [0.14285685122013092, 0.14285726845264435, 0.14285723865032196, 0.14285778999328613, 0.14285697042942047, 0.14285698533058167, 0.14285700023174286]\n",
      "extrinsic_reward :  0 intrinsic_reward :  411.218017578125\n",
      "place 43\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 43, 'x_pos_screen': 43, 'y_pos': 101}\n",
      "global_step :  16 , action :  6 action_prob :  [0.14285683631896973, 0.14285725355148315, 0.14285722374916077, 0.14285777509212494, 0.14285695552825928, 0.14285697042942047, 0.14285700023174286]\n",
      "extrinsic_reward :  0 intrinsic_reward :  410.66973876953125\n",
      "place 43\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 43, 'x_pos_screen': 43, 'y_pos': 99}\n",
      "global_step :  17 , action :  4 action_prob :  [0.14285683631896973, 0.14285725355148315, 0.14285722374916077, 0.14285777509212494, 0.14285695552825928, 0.14285698533058167, 0.14285700023174286]\n",
      "extrinsic_reward :  0 intrinsic_reward :  413.1494445800781\n",
      "place 43\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 43, 'x_pos_screen': 43, 'y_pos': 96}\n",
      "global_step :  18 , action :  3 action_prob :  [0.14285683631896973, 0.14285725355148315, 0.14285722374916077, 0.14285777509212494, 0.14285695552825928, 0.14285697042942047, 0.14285700023174286]\n",
      "extrinsic_reward :  1 intrinsic_reward :  412.9725341796875\n",
      "place 44\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 44, 'x_pos_screen': 44, 'y_pos': 93}\n",
      "global_step :  19 , action :  2 action_prob :  [0.14285685122013092, 0.14285726845264435, 0.14285723865032196, 0.14285778999328613, 0.14285697042942047, 0.14285698533058167, 0.14285700023174286]\n",
      "extrinsic_reward :  -1 intrinsic_reward :  414.2243957519531\n",
      "place 44\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 399, 'world': 1, 'x_pos': 44, 'x_pos_screen': 44, 'y_pos': 89}\n",
      "global_step :  20 , action :  2 action_prob :  [0.14285685122013092, 0.14285725355148315, 0.14285722374916077, 0.14285778999328613, 0.14285697042942047, 0.14285698533058167, 0.14285700023174286]\n",
      "extrinsic_reward :  0 intrinsic_reward :  411.939453125\n",
      "place 44\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 399, 'world': 1, 'x_pos': 44, 'x_pos_screen': 44, 'y_pos': 85}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step :  21 , action :  0 action_prob :  [0.1459439992904663, 0.14143623411655426, 0.13868798315525055, 0.14606451988220215, 0.14062733948230743, 0.14789240062236786, 0.13934753835201263]\n",
      "extrinsic_reward :  1 intrinsic_reward :  114.90711975097656\n",
      "place 45\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 399, 'world': 1, 'x_pos': 45, 'x_pos_screen': 45, 'y_pos': 79}\n",
      "global_step :  22 , action :  6 action_prob :  [0.14594414830207825, 0.1414361596107483, 0.13868775963783264, 0.14606468379497528, 0.14062722027301788, 0.14789265394210815, 0.1393473595380783]\n",
      "extrinsic_reward :  0 intrinsic_reward :  116.23658752441406\n",
      "place 45\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 399, 'world': 1, 'x_pos': 45, 'x_pos_screen': 45, 'y_pos': 79}\n",
      "global_step :  23 , action :  4 action_prob :  [0.14594340324401855, 0.14143650233745575, 0.1386888176202774, 0.146063894033432, 0.14062777161598206, 0.14789138734340668, 0.13934825360774994]\n",
      "extrinsic_reward :  0 intrinsic_reward :  116.73284149169922\n",
      "place 45\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 399, 'world': 1, 'x_pos': 45, 'x_pos_screen': 45, 'y_pos': 79}\n",
      "global_step :  24 , action :  1 action_prob :  [0.14594385027885437, 0.14143630862236023, 0.13868819177150726, 0.14606435596942902, 0.1406274437904358, 0.14789214730262756, 0.13934773206710815]\n",
      "extrinsic_reward :  0 intrinsic_reward :  116.96817016601562\n",
      "place 45\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 399, 'world': 1, 'x_pos': 45, 'x_pos_screen': 45, 'y_pos': 84}\n",
      "global_step :  25 , action :  0 action_prob :  [0.1459435224533081, 0.14143644273281097, 0.13868862390518188, 0.14606402814388275, 0.1406276673078537, 0.14789161086082458, 0.1393480747938156]\n",
      "extrinsic_reward :  0 intrinsic_reward :  116.32644653320312\n",
      "place 45\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 399, 'world': 1, 'x_pos': 45, 'x_pos_screen': 45, 'y_pos': 89}\n",
      "global_step :  26 , action :  0 action_prob :  [0.14594407379627228, 0.14143621921539307, 0.1386878937482834, 0.14606459438800812, 0.14062729477882385, 0.1478925198316574, 0.13934747874736786]\n",
      "extrinsic_reward :  0 intrinsic_reward :  116.05663299560547\n",
      "place 45\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 399, 'world': 1, 'x_pos': 45, 'x_pos_screen': 45, 'y_pos': 93}\n",
      "global_step :  27 , action :  6 action_prob :  [0.14594362676143646, 0.1414364129304886, 0.13868850469589233, 0.1460641473531723, 0.14062760770320892, 0.14789177477359772, 0.13934798538684845]\n",
      "extrinsic_reward :  1 intrinsic_reward :  116.66691589355469\n",
      "place 46\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 399, 'world': 1, 'x_pos': 46, 'x_pos_screen': 46, 'y_pos': 96}\n",
      "global_step :  28 , action :  5 action_prob :  [0.14594314992427826, 0.1414366066455841, 0.13868913054466248, 0.1460636407136917, 0.1406279355287552, 0.14789097011089325, 0.13934850692749023]\n",
      "extrinsic_reward :  -1 intrinsic_reward :  115.47328186035156\n",
      "place 45\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 399, 'world': 1, 'x_pos': 45, 'x_pos_screen': 45, 'y_pos': 99}\n",
      "global_step :  29 , action :  3 action_prob :  [0.14594191312789917, 0.14143718779087067, 0.13869090378284454, 0.14606234431266785, 0.1406288743019104, 0.14788883924484253, 0.13934999704360962]\n",
      "extrinsic_reward :  0 intrinsic_reward :  116.5968017578125\n",
      "place 45\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 399, 'world': 1, 'x_pos': 45, 'x_pos_screen': 45, 'y_pos': 101}\n",
      "global_step :  30 , action :  1 action_prob :  [0.14594164490699768, 0.14143729209899902, 0.1386912316083908, 0.14606207609176636, 0.14062903821468353, 0.1478884071111679, 0.1393502652645111]\n",
      "extrinsic_reward :  0 intrinsic_reward :  116.26668548583984\n",
      "place 45\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 399, 'world': 1, 'x_pos': 45, 'x_pos_screen': 45, 'y_pos': 102}\n",
      "global_step :  31 , action :  6 action_prob :  [0.1459401398897171, 0.14143797755241394, 0.13869331777095795, 0.146060511469841, 0.14063015580177307, 0.14788585901260376, 0.1393520087003708]\n",
      "extrinsic_reward :  0 intrinsic_reward :  116.1148681640625\n",
      "place 45\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 399, 'world': 1, 'x_pos': 45, 'x_pos_screen': 45, 'y_pos': 103}\n",
      "global_step :  32 , action :  6 action_prob :  [0.1459394097328186, 0.1414383053779602, 0.13869434595108032, 0.14605975151062012, 0.14063069224357605, 0.14788462221622467, 0.13935287296772003]\n",
      "extrinsic_reward :  0 intrinsic_reward :  117.24345397949219\n",
      "place 45\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 399, 'world': 1, 'x_pos': 45, 'x_pos_screen': 45, 'y_pos': 103}\n",
      "global_step :  33 , action :  3 action_prob :  [0.14593897759914398, 0.14143849909305573, 0.13869494199752808, 0.1460593193769455, 0.14063100516796112, 0.14788389205932617, 0.13935337960720062]\n",
      "extrinsic_reward :  0 intrinsic_reward :  117.42820739746094\n",
      "place 45\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 399, 'world': 1, 'x_pos': 45, 'x_pos_screen': 45, 'y_pos': 102}\n",
      "global_step :  34 , action :  2 action_prob :  [0.14593863487243652, 0.14143867790699005, 0.13869546353816986, 0.14605893194675446, 0.1406312882900238, 0.14788328111171722, 0.13935381174087524]\n",
      "extrinsic_reward :  0 intrinsic_reward :  116.30136108398438\n",
      "place 45\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 399, 'world': 1, 'x_pos': 45, 'x_pos_screen': 45, 'y_pos': 101}\n",
      "global_step :  35 , action :  4 action_prob :  [0.14593902230262756, 0.14143846929073334, 0.1386948525905609, 0.14605934917926788, 0.14063094556331635, 0.14788398146629333, 0.13935327529907227]\n",
      "extrinsic_reward :  0 intrinsic_reward :  116.4520263671875\n",
      "place 45\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 399, 'world': 1, 'x_pos': 45, 'x_pos_screen': 45, 'y_pos': 99}\n",
      "global_step :  36 , action :  1 action_prob :  [0.14593976736068726, 0.14143812656402588, 0.13869382441043854, 0.14606010913848877, 0.14063040912151337, 0.14788523316383362, 0.1393524408340454]\n",
      "extrinsic_reward :  0 intrinsic_reward :  116.78179931640625\n",
      "place 45\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 399, 'world': 1, 'x_pos': 45, 'x_pos_screen': 45, 'y_pos': 96}\n",
      "global_step :  37 , action :  0 action_prob :  [0.14594128727912903, 0.14143745601177216, 0.1386917531490326, 0.14606168866157532, 0.14062932133674622, 0.14788779616355896, 0.13935069739818573]\n",
      "extrinsic_reward :  0 intrinsic_reward :  116.04119110107422\n",
      "place 45\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 399, 'world': 1, 'x_pos': 45, 'x_pos_screen': 45, 'y_pos': 93}\n",
      "global_step :  38 , action :  0 action_prob :  [0.14594221115112305, 0.14143703877925873, 0.13869045674800873, 0.1460626721382141, 0.1406286358833313, 0.1478893756866455, 0.13934962451457977]\n",
      "extrinsic_reward :  1 intrinsic_reward :  116.87062072753906\n",
      "place 46\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 399, 'world': 1, 'x_pos': 46, 'x_pos_screen': 46, 'y_pos': 89}\n",
      "global_step :  39 , action :  3 action_prob :  [0.1459427773952484, 0.14143677055835724, 0.13868963718414307, 0.14606325328350067, 0.14062820374965668, 0.1478903442621231, 0.13934892416000366]\n",
      "extrinsic_reward :  -1 intrinsic_reward :  116.42130279541016\n",
      "place 46\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 398, 'world': 1, 'x_pos': 46, 'x_pos_screen': 46, 'y_pos': 85}\n",
      "global_step :  40 , action :  4 action_prob :  [0.14594332873821259, 0.14143654704093933, 0.13868892192840576, 0.14606381952762604, 0.14062783122062683, 0.14789125323295593, 0.1393483430147171]\n",
      "extrinsic_reward :  0 intrinsic_reward :  116.07467651367188\n",
      "place 46\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 398, 'world': 1, 'x_pos': 46, 'x_pos_screen': 46, 'y_pos': 79}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step :  41 , action :  4 action_prob :  [0.13961932063102722, 0.1379391849040985, 0.1424020677804947, 0.14805886149406433, 0.1457410603761673, 0.1543506383895874, 0.13188888132572174]\n",
      "extrinsic_reward :  1 intrinsic_reward :  93.4362564086914\n",
      "place 47\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 398, 'world': 1, 'x_pos': 47, 'x_pos_screen': 47, 'y_pos': 84}\n",
      "global_step :  42 , action :  4 action_prob :  [0.13961975276470184, 0.13793976604938507, 0.14240212738513947, 0.14805828034877777, 0.14574070274829865, 0.15434928238391876, 0.13189013302326202]\n",
      "extrinsic_reward :  0 intrinsic_reward :  93.27044677734375\n",
      "place 47\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 398, 'world': 1, 'x_pos': 47, 'x_pos_screen': 47, 'y_pos': 89}\n",
      "global_step :  43 , action :  0 action_prob :  [0.1396188586950302, 0.13793857395648956, 0.14240199327468872, 0.14805945754051208, 0.14574141800403595, 0.1543520838022232, 0.1318875402212143]\n",
      "extrinsic_reward :  1 intrinsic_reward :  93.60057830810547\n",
      "place 48\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 398, 'world': 1, 'x_pos': 48, 'x_pos_screen': 48, 'y_pos': 93}\n",
      "global_step :  44 , action :  1 action_prob :  [0.1396193951368332, 0.13793928921222687, 0.14240208268165588, 0.14805875718593597, 0.14574100077152252, 0.1543504148721695, 0.13188908994197845]\n",
      "extrinsic_reward :  1 intrinsic_reward :  92.96177673339844\n",
      "place 49\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 398, 'world': 1, 'x_pos': 49, 'x_pos_screen': 49, 'y_pos': 98}\n",
      "global_step :  45 , action :  4 action_prob :  [0.139620840549469, 0.13794125616550446, 0.1424022763967514, 0.1480567902326584, 0.14573977887630463, 0.15434570610523224, 0.13189339637756348]\n",
      "extrinsic_reward :  0 intrinsic_reward :  92.82881927490234\n",
      "place 49\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 398, 'world': 1, 'x_pos': 49, 'x_pos_screen': 49, 'y_pos': 101}\n",
      "global_step :  46 , action :  3 action_prob :  [0.1396234631538391, 0.13794481754302979, 0.14240260422229767, 0.14805318415164948, 0.14573754370212555, 0.15433716773986816, 0.13190117478370667]\n",
      "extrinsic_reward :  1 intrinsic_reward :  92.3698959350586\n",
      "place 50\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 398, 'world': 1, 'x_pos': 50, 'x_pos_screen': 50, 'y_pos': 104}\n",
      "global_step :  47 , action :  1 action_prob :  [0.13962547481060028, 0.13794752955436707, 0.14240287244319916, 0.1480504721403122, 0.14573587477207184, 0.15433068573474884, 0.131907120347023]\n",
      "extrinsic_reward :  1 intrinsic_reward :  92.88418579101562\n",
      "place 51\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 398, 'world': 1, 'x_pos': 51, 'x_pos_screen': 51, 'y_pos': 106}\n",
      "global_step :  48 , action :  6 action_prob :  [0.13962684571743011, 0.1379493921995163, 0.1424030363559723, 0.14804857969284058, 0.14573471248149872, 0.15432621538639069, 0.13191118836402893]\n",
      "extrinsic_reward :  2 intrinsic_reward :  93.02242279052734\n",
      "place 53\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 398, 'world': 1, 'x_pos': 53, 'x_pos_screen': 53, 'y_pos': 108}\n",
      "global_step :  49 , action :  5 action_prob :  [0.13962718844413757, 0.1379498541355133, 0.14240309596061707, 0.14804813265800476, 0.14573444426059723, 0.15432514250278473, 0.13191218674182892]\n",
      "extrinsic_reward :  1 intrinsic_reward :  93.59890747070312\n",
      "place 54\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 398, 'world': 1, 'x_pos': 54, 'x_pos_screen': 54, 'y_pos': 109}\n",
      "global_step :  50 , action :  4 action_prob :  [0.13962611556053162, 0.1379484087228775, 0.14240296185016632, 0.14804957807064056, 0.14573533833026886, 0.1543285846710205, 0.13190902769565582]\n",
      "extrinsic_reward :  1 intrinsic_reward :  93.92987060546875\n",
      "place 55\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 398, 'world': 1, 'x_pos': 55, 'x_pos_screen': 55, 'y_pos': 109}\n",
      "global_step :  51 , action :  0 action_prob :  [0.13962715864181519, 0.13794982433319092, 0.14240308105945587, 0.14804814755916595, 0.14573444426059723, 0.15432515740394592, 0.13191214203834534]\n",
      "extrinsic_reward :  1 intrinsic_reward :  93.89519500732422\n",
      "place 56\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 398, 'world': 1, 'x_pos': 56, 'x_pos_screen': 56, 'y_pos': 109}\n",
      "global_step :  52 , action :  6 action_prob :  [0.1396271139383316, 0.13794974982738495, 0.14240306615829468, 0.14804820716381073, 0.14573447406291962, 0.15432532131671906, 0.1319120079278946]\n",
      "extrinsic_reward :  1 intrinsic_reward :  93.47480773925781\n",
      "place 57\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 398, 'world': 1, 'x_pos': 57, 'x_pos_screen': 57, 'y_pos': 108}\n",
      "global_step :  53 , action :  5 action_prob :  [0.1396263986825943, 0.13794879615306854, 0.14240297675132751, 0.14804916083812714, 0.14573507010936737, 0.1543276011943817, 0.13190990686416626]\n",
      "extrinsic_reward :  1 intrinsic_reward :  93.94559478759766\n",
      "place 58\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 398, 'world': 1, 'x_pos': 58, 'x_pos_screen': 58, 'y_pos': 106}\n",
      "global_step :  54 , action :  4 action_prob :  [0.1396266222000122, 0.13794907927513123, 0.1424030065536499, 0.14804887771606445, 0.14573489129543304, 0.154326930642128, 0.1319105327129364]\n",
      "extrinsic_reward :  1 intrinsic_reward :  94.7466049194336\n",
      "place 59\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 398, 'world': 1, 'x_pos': 59, 'x_pos_screen': 59, 'y_pos': 104}\n",
      "global_step :  55 , action :  4 action_prob :  [0.1396273821592331, 0.1379501074552536, 0.14240311086177826, 0.14804787933826447, 0.1457342654466629, 0.1543245166540146, 0.13191275298595428]\n",
      "extrinsic_reward :  1 intrinsic_reward :  94.14144897460938\n",
      "place 60\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 398, 'world': 1, 'x_pos': 60, 'x_pos_screen': 60, 'y_pos': 101}\n",
      "global_step :  56 , action :  5 action_prob :  [0.13962750136852264, 0.13795028626918793, 0.14240312576293945, 0.14804768562316895, 0.14573414623737335, 0.15432406961917877, 0.13191315531730652]\n",
      "extrinsic_reward :  2 intrinsic_reward :  92.96104431152344\n",
      "place 62\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 398, 'world': 1, 'x_pos': 62, 'x_pos_screen': 62, 'y_pos': 97}\n",
      "global_step :  57 , action :  0 action_prob :  [0.13962651789188385, 0.13794894516468048, 0.1424029916524887, 0.1480490267276764, 0.145734965801239, 0.15432727336883545, 0.13191021978855133]\n",
      "extrinsic_reward :  1 intrinsic_reward :  94.14058685302734\n",
      "place 63\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 398, 'world': 1, 'x_pos': 63, 'x_pos_screen': 63, 'y_pos': 93}\n",
      "global_step :  58 , action :  1 action_prob :  [0.13962776958942413, 0.13795064389705658, 0.14240315556526184, 0.1480473428964615, 0.14573393762111664, 0.15432323515415192, 0.1319139301776886]\n",
      "extrinsic_reward :  1 intrinsic_reward :  93.27978515625\n",
      "place 64\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 398, 'world': 1, 'x_pos': 64, 'x_pos_screen': 64, 'y_pos': 88}\n",
      "global_step :  59 , action :  2 action_prob :  [0.13962718844413757, 0.1379498541355133, 0.14240308105945587, 0.14804811775684357, 0.14573441445827484, 0.15432509779930115, 0.1319122165441513]\n",
      "extrinsic_reward :  1 intrinsic_reward :  95.12744903564453\n",
      "place 66\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 397, 'world': 1, 'x_pos': 66, 'x_pos_screen': 66, 'y_pos': 83}\n",
      "global_step :  60 , action :  3 action_prob :  [0.13962669670581818, 0.13794919848442078, 0.1424030214548111, 0.1480487585067749, 0.14573481678962708, 0.1543266475200653, 0.1319107860326767]\n",
      "extrinsic_reward :  1 intrinsic_reward :  95.41568756103516\n",
      "place 67\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 397, 'world': 1, 'x_pos': 67, 'x_pos_screen': 67, 'y_pos': 79}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step :  61 , action :  1 action_prob :  [0.14050406217575073, 0.14339980483055115, 0.13978461921215057, 0.14273959398269653, 0.14994189143180847, 0.15096984803676605, 0.1326601356267929]\n",
      "extrinsic_reward :  2 intrinsic_reward :  61.74724578857422\n",
      "place 69\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 397, 'world': 1, 'x_pos': 69, 'x_pos_screen': 69, 'y_pos': 79}\n",
      "global_step :  62 , action :  0 action_prob :  [0.1405046135187149, 0.1433996856212616, 0.13978523015975952, 0.14273971319198608, 0.1499403715133667, 0.15096823871135712, 0.13266213238239288]\n",
      "extrinsic_reward :  2 intrinsic_reward :  61.314208984375\n",
      "place 71\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 397, 'world': 1, 'x_pos': 71, 'x_pos_screen': 71, 'y_pos': 79}\n",
      "global_step :  63 , action :  1 action_prob :  [0.1405055820941925, 0.1433994621038437, 0.13978633284568787, 0.1427398920059204, 0.14993765950202942, 0.15096533298492432, 0.1326657235622406]\n",
      "extrinsic_reward :  2 intrinsic_reward :  61.52710723876953\n",
      "place 73\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 397, 'world': 1, 'x_pos': 73, 'x_pos_screen': 73, 'y_pos': 79}\n",
      "global_step :  64 , action :  5 action_prob :  [0.14050474762916565, 0.1433996558189392, 0.13978537917137146, 0.14273974299430847, 0.14993996918201447, 0.1509678214788437, 0.13266265392303467]\n",
      "extrinsic_reward :  2 intrinsic_reward :  61.510746002197266\n",
      "place 75\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 397, 'world': 1, 'x_pos': 75, 'x_pos_screen': 75, 'y_pos': 79}\n",
      "global_step :  65 , action :  0 action_prob :  [0.14050564169883728, 0.1433994323015213, 0.13978642225265503, 0.1427399069070816, 0.1499374657869339, 0.1509651243686676, 0.1326659768819809]\n",
      "extrinsic_reward :  2 intrinsic_reward :  61.08636474609375\n",
      "place 77\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 397, 'world': 1, 'x_pos': 77, 'x_pos_screen': 77, 'y_pos': 85}\n",
      "global_step :  66 , action :  3 action_prob :  [0.14050540328025818, 0.14339950680732727, 0.13978613913059235, 0.14273986220359802, 0.14993816614151, 0.1509658694267273, 0.13266505300998688]\n",
      "extrinsic_reward :  2 intrinsic_reward :  61.04024887084961\n",
      "place 79\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 397, 'world': 1, 'x_pos': 79, 'x_pos_screen': 79, 'y_pos': 91}\n",
      "global_step :  67 , action :  4 action_prob :  [0.14050617814064026, 0.14339931309223175, 0.13978703320026398, 0.14274001121520996, 0.14993597567081451, 0.15096351504325867, 0.13266795873641968]\n",
      "extrinsic_reward :  2 intrinsic_reward :  60.85041427612305\n",
      "place 81\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 397, 'world': 1, 'x_pos': 81, 'x_pos_screen': 81, 'y_pos': 96}\n",
      "global_step :  68 , action :  5 action_prob :  [0.1405065655708313, 0.1433992236852646, 0.1397874802350998, 0.14274010062217712, 0.14993490278720856, 0.15096238255500793, 0.1326693892478943]\n",
      "extrinsic_reward :  2 intrinsic_reward :  60.680572509765625\n",
      "place 83\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 397, 'world': 1, 'x_pos': 83, 'x_pos_screen': 82, 'y_pos': 100}\n",
      "global_step :  69 , action :  3 action_prob :  [0.14050772786140442, 0.1433989256620407, 0.13978880643844604, 0.14274030923843384, 0.14993159472942352, 0.150958850979805, 0.1326737403869629]\n",
      "extrinsic_reward :  2 intrinsic_reward :  60.880516052246094\n",
      "place 85\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 397, 'world': 1, 'x_pos': 85, 'x_pos_screen': 83, 'y_pos': 104}\n",
      "global_step :  70 , action :  4 action_prob :  [0.1405302882194519, 0.14339357614517212, 0.13981448113918304, 0.14274467527866364, 0.1498684138059616, 0.15089111030101776, 0.1327574998140335]\n",
      "extrinsic_reward :  2 intrinsic_reward :  60.5516357421875\n",
      "place 87\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 397, 'world': 1, 'x_pos': 87, 'x_pos_screen': 84, 'y_pos': 106}\n",
      "global_step :  71 , action :  6 action_prob :  [0.14051733911037445, 0.14339669048786163, 0.13979969918727875, 0.14274215698242188, 0.1499047428369522, 0.1509300321340561, 0.13270936906337738]\n",
      "extrinsic_reward :  2 intrinsic_reward :  58.510765075683594\n",
      "place 89\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 397, 'world': 1, 'x_pos': 89, 'x_pos_screen': 85, 'y_pos': 108}\n",
      "global_step :  72 , action :  6 action_prob :  [0.14052581787109375, 0.14339463412761688, 0.13980937004089355, 0.1427437961101532, 0.14988088607788086, 0.15090450644493103, 0.13274091482162476]\n",
      "extrinsic_reward :  3 intrinsic_reward :  60.50233459472656\n",
      "place 92\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 397, 'world': 1, 'x_pos': 92, 'x_pos_screen': 86, 'y_pos': 109}\n",
      "global_step :  73 , action :  3 action_prob :  [0.1405222862958908, 0.14339549839496613, 0.13980534672737122, 0.1427430957555771, 0.14989082515239716, 0.15091511607170105, 0.13272780179977417]\n",
      "extrinsic_reward :  2 intrinsic_reward :  61.027320861816406\n",
      "place 94\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 397, 'world': 1, 'x_pos': 94, 'x_pos_screen': 87, 'y_pos': 109}\n",
      "global_step :  74 , action :  5 action_prob :  [0.14053195714950562, 0.14339318871498108, 0.13981635868549347, 0.1427449882030487, 0.14986370503902435, 0.15088607370853424, 0.13276371359825134]\n",
      "extrinsic_reward :  2 intrinsic_reward :  61.57197570800781\n",
      "place 96\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 397, 'world': 1, 'x_pos': 96, 'x_pos_screen': 88, 'y_pos': 108}\n",
      "global_step :  75 , action :  5 action_prob :  [0.14053449034690857, 0.14339259266853333, 0.1398192197084427, 0.14274543523788452, 0.14985662698745728, 0.1508784145116806, 0.13277314603328705]\n",
      "extrinsic_reward :  2 intrinsic_reward :  61.00938415527344\n",
      "place 98\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 397, 'world': 1, 'x_pos': 98, 'x_pos_screen': 89, 'y_pos': 107}\n",
      "global_step :  76 , action :  4 action_prob :  [0.1405288428068161, 0.14339393377304077, 0.13981281220912933, 0.14274434745311737, 0.14987246692180634, 0.1508953869342804, 0.13275215029716492]\n",
      "extrinsic_reward :  3 intrinsic_reward :  59.68172073364258\n",
      "place 101\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 397, 'world': 1, 'x_pos': 101, 'x_pos_screen': 90, 'y_pos': 104}\n",
      "global_step :  77 , action :  0 action_prob :  [0.1405440866947174, 0.14339028298854828, 0.13983015716075897, 0.14274731278419495, 0.14982970058918, 0.1508496105670929, 0.1328088492155075]\n",
      "extrinsic_reward :  2 intrinsic_reward :  62.668331146240234\n",
      "place 103\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 397, 'world': 1, 'x_pos': 103, 'x_pos_screen': 91, 'y_pos': 101}\n",
      "global_step :  78 , action :  6 action_prob :  [0.14053292572498322, 0.14339296519756317, 0.13981741666793823, 0.14274516701698303, 0.14986100792884827, 0.15088316798210144, 0.13276733458042145]\n",
      "extrinsic_reward :  2 intrinsic_reward :  63.22275161743164\n",
      "place 105\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 397, 'world': 1, 'x_pos': 105, 'x_pos_screen': 92, 'y_pos': 97}\n",
      "global_step :  79 , action :  2 action_prob :  [0.1405254602432251, 0.14339475333690643, 0.13980890810489655, 0.14274369180202484, 0.1498819887638092, 0.15090560913085938, 0.13273954391479492]\n",
      "extrinsic_reward :  2 intrinsic_reward :  62.66920471191406\n",
      "place 108\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 396, 'world': 1, 'x_pos': 108, 'x_pos_screen': 93, 'y_pos': 92}\n",
      "global_step :  80 , action :  3 action_prob :  [0.14053751528263092, 0.14339189231395721, 0.13982261717319489, 0.14274603128433228, 0.14984820783138275, 0.1508694291114807, 0.13278430700302124]\n",
      "extrinsic_reward :  2 intrinsic_reward :  65.77687072753906\n",
      "place 110\n",
      "info {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 396, 'world': 1, 'x_pos': 110, 'x_pos_screen': 94, 'y_pos': 87}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-35aa60dff265>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m             \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m     \u001b[1;31m#env.render()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-a41667b60a3f>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\unity\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \"\"\"\n\u001b[1;32m--> 102\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\unity\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "T_horizon = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    global_step = 0\n",
    "    state = env.reset()\n",
    "    state = np.array(state)/255\n",
    "            #state = np.transpose(state,(2,0,1))\n",
    "    state = np.moveaxis(state, -1, 0)\n",
    "    state = torch.tensor(state).float()\n",
    "    state = state.unsqueeze(0)\n",
    "    done = False\n",
    "    while not done :\n",
    "        for t in range(T_horizon):\n",
    "            #env.render()\n",
    "            global_step +=1\n",
    "\n",
    "            if gpu:\n",
    "                action_prob, _ , _ = model.ppo.forward(state.cuda())\n",
    "            else:\n",
    "                action_prob, _ , _ = model.ppo.forward(state)\n",
    "            m = Categorical(action_prob)\n",
    "            action = m.sample().item()\n",
    "            next_state, extrinsic_reward, done, info = env.step(action)\n",
    "            next_state = np.array(next_state)/255\n",
    "            next_state = np.moveaxis(next_state,-1,0)\n",
    "            next_state = torch.tensor(next_state).float()\n",
    "            next_state = next_state.unsqueeze(0)\n",
    "            if gpu:\n",
    "                predictor,target = model.rnd.forward(next_state.cuda())\n",
    "            else:\n",
    "                predictor,target = model.rnd.forward(next_state)\n",
    "            intrinsic_reward = (predictor - target).pow(2).sum(1) / 2\n",
    "            if info['time'] == 0 :\n",
    "                done = True\n",
    "                reward = -10.\n",
    "\n",
    "            model.put_data((state.tolist(), action, extrinsic_reward/100, (intrinsic_reward.item())/10000, next_state.tolist(), action_prob[0][action].item(), done))\n",
    "            print('global_step : ',global_step,', action : ', action,'action_prob : ',action_prob.tolist()[0])\n",
    "            print('extrinsic_reward : ',extrinsic_reward,'intrinsic_reward : ',intrinsic_reward.item())\n",
    "            print('place',info['x_pos'])\n",
    "            print('info',info)\n",
    "            if done :\n",
    "                print('epoch : ',epoch, ', global_step : ',global_step)\n",
    "                break\n",
    "            state = next_state\n",
    "        model.train()\n",
    "    #env.render()\n",
    "\n",
    "#env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'coins': 0,\n",
       " 'flag_get': False,\n",
       " 'life': 2,\n",
       " 'score': 0,\n",
       " 'stage': 1,\n",
       " 'status': 'small',\n",
       " 'time': 311,\n",
       " 'world': 1,\n",
       " 'x_pos': 594,\n",
       " 'x_pos_screen': 85,\n",
       " 'y_pos': 79}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "319"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "env has already been closed.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-1baceacf4cb1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\unity\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mclose\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 236\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    237\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\unity\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mclose\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 236\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    237\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\unity\\lib\\site-packages\\nes_py\\nes_env.py\u001b[0m in \u001b[0;36mclose\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    336\u001b[0m         \u001b[1;31m# make sure the environment hasn't already been closed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_env\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 338\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'env has already been closed.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    339\u001b[0m         \u001b[1;31m# purge the environment from C++ memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m         \u001b[0m_LIB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mClose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_env\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: env has already been closed."
     ]
    }
   ],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "if gpu:\n",
    "    model.load_state_dict(torch.load(\"weights/1_episode_model\"))\n",
    "else:\n",
    "    model.load_state_dict(torch.load(\"weights/1_episode_model\", map_location={'cuda:0': 'cpu'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-1a8884e7e164>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m                 \u001b[0maction_prob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mppo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m                 \u001b[0maction_prob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mppo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m             \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCategorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction_prob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-a84c4e972846>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, dim)\u001b[0m\n\u001b[0;32m     53\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[0maction_prob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\unity\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\unity\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\unity\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\unity\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    318\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[1;32m--> 320\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    321\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "T_horizon = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    global_step = 0\n",
    "    state = env.reset()\n",
    "    state = np.array(state)/255\n",
    "            #state = np.transpose(state,(2,0,1))\n",
    "    state = np.moveaxis(state, -1, 0)\n",
    "    state = torch.tensor(state).float()\n",
    "    state = state.unsqueeze(0)\n",
    "    done = False\n",
    "    while not done :\n",
    "        for t in range(T_horizon):\n",
    "            env.render()\n",
    "            global_step +=1\n",
    "\n",
    "            if gpu:\n",
    "                action_prob, _ , _ = model.ppo.forward(state.cuda())\n",
    "            else:\n",
    "                action_prob, _ , _ = model.ppo.forward(state)\n",
    "            m = Categorical(action_prob)\n",
    "            action = m.sample().item()\n",
    "            next_state, extrinsic_reward, done, info = env.step(action)\n",
    "            next_state = np.array(next_state)/255\n",
    "            next_state = np.moveaxis(next_state,-1,0)\n",
    "            next_state = torch.tensor(next_state).float()\n",
    "            next_state = next_state.unsqueeze(0)\n",
    "            if gpu:\n",
    "                predictor,target = model.rnd.forward(next_state.cuda())\n",
    "            else:\n",
    "                predictor,target = model.rnd.forward(next_state)\n",
    "            intrinsic_reward = (predictor - target).pow(2).sum(1) / 2\n",
    "            if info['time'] == 0 :\n",
    "                done = True\n",
    "                reward = -10.\n",
    "            if done :\n",
    "                print('epoch : ',epoch, ', global_step : ',global_step)\n",
    "                break\n",
    "            state = next_state\n",
    "    #env.render()\n",
    "\n",
    "#env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unity",
   "language": "python",
   "name": "unity"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
