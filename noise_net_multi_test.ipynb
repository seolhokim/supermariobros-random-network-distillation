{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from torch.nn import init\n",
    "import copy\n",
    "import numpy as np\n",
    "import math\n",
    "import queue\n",
    "from IPython.display import clear_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyLinear(nn.Module):\n",
    "    \"\"\"Factorised Gaussian NoisyNet\"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, sigma0=0.5):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.bias = nn.Parameter(torch.Tensor(out_features))\n",
    "        self.noisy_weight = nn.Parameter(\n",
    "            torch.Tensor(out_features, in_features))\n",
    "        self.noisy_bias = nn.Parameter(torch.Tensor(out_features))\n",
    "        self.noise_std = sigma0 / math.sqrt(self.in_features)\n",
    "\n",
    "        self.reset_parameters()\n",
    "        self.register_noise()\n",
    "\n",
    "    def register_noise(self):\n",
    "        in_noise = torch.FloatTensor(self.in_features)\n",
    "        out_noise = torch.FloatTensor(self.out_features)\n",
    "        noise = torch.FloatTensor(self.out_features, self.in_features)\n",
    "        self.register_buffer('in_noise', in_noise)\n",
    "        self.register_buffer('out_noise', out_noise)\n",
    "        self.register_buffer('noise', noise)\n",
    "\n",
    "    def sample_noise(self):\n",
    "        self.in_noise.normal_(0, self.noise_std)\n",
    "        self.out_noise.normal_(0, self.noise_std)\n",
    "        self.noise = torch.mm(\n",
    "            self.out_noise.view(-1, 1), self.in_noise.view(1, -1))\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        self.noisy_weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "            self.noisy_bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Note: noise will be updated if x is not volatile\n",
    "        \"\"\"\n",
    "        normal_y = nn.functional.linear(x, self.weight, self.bias)\n",
    "        if self.training:\n",
    "            # update the noise once per update\n",
    "            self.sample_noise()\n",
    "\n",
    "        noisy_weight = self.noisy_weight * self.noise\n",
    "        noisy_bias = self.noisy_bias * self.out_noise\n",
    "        noisy_y = nn.functional.linear(x, noisy_weight, noisy_bias)\n",
    "        return noisy_y + normal_y\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(' \\\n",
    "            + 'in_features=' + str(self.in_features) \\\n",
    "            + ', out_features=' + str(self.out_features) + ')'\n",
    "\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "\n",
    "class PPO(nn.Module):\n",
    "    def __init__(self, input_size, output_size, use_noisy_net=True):\n",
    "        super(CnnActorCriticNetwork, self).__init__()\n",
    "\n",
    "        if use_noisy_net:\n",
    "            print('use NoisyNet')\n",
    "            linear = NoisyLinear\n",
    "        else:\n",
    "            linear = nn.Linear\n",
    "\n",
    "        self.feature = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=4,\n",
    "                out_channels=32,\n",
    "                kernel_size=8,\n",
    "                stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(\n",
    "                in_channels=32,\n",
    "                out_channels=64,\n",
    "                kernel_size=4,\n",
    "                stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(\n",
    "                in_channels=64,\n",
    "                out_channels=64,\n",
    "                kernel_size=3,\n",
    "                stride=1),\n",
    "            nn.ReLU(),\n",
    "            Flatten(),\n",
    "            linear(\n",
    "                7 * 7 * 64,\n",
    "                256),\n",
    "            nn.ReLU(),\n",
    "            linear(\n",
    "                256,\n",
    "                448),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.actor = nn.Sequential(\n",
    "            linear(448, 448),\n",
    "            nn.ReLU(),\n",
    "            linear(448, output_size)\n",
    "        )\n",
    "\n",
    "        self.extra_layer = nn.Sequential(\n",
    "            linear(448, 448),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.critic_ext = linear(448, 1)\n",
    "        self.critic_int = linear(448, 1)\n",
    "\n",
    "        for p in self.modules():\n",
    "            if isinstance(p, nn.Conv2d):\n",
    "                init.orthogonal_(p.weight, np.sqrt(2))\n",
    "                p.bias.data.zero_()\n",
    "\n",
    "            if isinstance(p, nn.Linear):\n",
    "                init.orthogonal_(p.weight, np.sqrt(2))\n",
    "                p.bias.data.zero_()\n",
    "\n",
    "        init.orthogonal_(self.critic_ext.weight, 0.01)\n",
    "        self.critic_ext.bias.data.zero_()\n",
    "\n",
    "        init.orthogonal_(self.critic_int.weight, 0.01)\n",
    "        self.critic_int.bias.data.zero_()\n",
    "\n",
    "        for i in range(len(self.actor)):\n",
    "            if type(self.actor[i]) == nn.Linear:\n",
    "                init.orthogonal_(self.actor[i].weight, 0.01)\n",
    "                self.actor[i].bias.data.zero_()\n",
    "\n",
    "        for i in range(len(self.extra_layer)):\n",
    "            if type(self.extra_layer[i]) == nn.Linear:\n",
    "                init.orthogonal_(self.extra_layer[i].weight, 0.1)\n",
    "                self.extra_layer[i].bias.data.zero_()\n",
    "\n",
    "    def forward(self, state,dim = -1):\n",
    "        x = self.feature(state)\n",
    "        policy = self.actor(x)\n",
    "        policy = F.softmax(policy,dim = dim)\n",
    "        value_ext = self.critic_ext(self.extra_layer(x) + x)\n",
    "        value_int = self.critic_int(self.extra_layer(x) + x)\n",
    "        return policy, value_ext, value_int\n",
    "\n",
    "\n",
    "class RND(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(RND, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        feature_output = 7 * 7 * 64\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=1,\n",
    "                out_channels=32,\n",
    "                kernel_size=8,\n",
    "                stride=4),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(\n",
    "                in_channels=32,\n",
    "                out_channels=64,\n",
    "                kernel_size=4,\n",
    "                stride=2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(\n",
    "                in_channels=64,\n",
    "                out_channels=64,\n",
    "                kernel_size=3,\n",
    "                stride=1),\n",
    "            nn.LeakyReLU(),\n",
    "            Flatten(),\n",
    "            nn.Linear(feature_output, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512)\n",
    "        )\n",
    "\n",
    "        self.target = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=1,\n",
    "                out_channels=32,\n",
    "                kernel_size=8,\n",
    "                stride=4),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(\n",
    "                in_channels=32,\n",
    "                out_channels=64,\n",
    "                kernel_size=4,\n",
    "                stride=2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(\n",
    "                in_channels=64,\n",
    "                out_channels=64,\n",
    "                kernel_size=3,\n",
    "                stride=1),\n",
    "            nn.LeakyReLU(),\n",
    "            Flatten(),\n",
    "            nn.Linear(feature_output, 512)\n",
    "        )\n",
    "\n",
    "        for p in self.modules():\n",
    "            if isinstance(p, nn.Conv2d):\n",
    "                init.orthogonal_(p.weight, np.sqrt(2))\n",
    "                p.bias.data.zero_()\n",
    "\n",
    "            if isinstance(p, nn.Linear):\n",
    "                init.orthogonal_(p.weight, np.sqrt(2))\n",
    "                p.bias.data.zero_()\n",
    "\n",
    "        for param in self.target.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, next_obs):\n",
    "        target_feature = self.target(next_obs)\n",
    "        predict_feature = self.predictor(next_obs)\n",
    "\n",
    "        return predict_feature, target_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "gamma         = 0.98\n",
    "lmbda         = 0.95\n",
    "eps_clip      = 0.1\n",
    "K_epoch       = 4\n",
    "T_horizon     = 128\n",
    "critic_coef = 0.5\n",
    "ent_coef = 0.001\n",
    "intrinsic_gamma = 0.99\n",
    "extrinsic_gamma = 0.999\n",
    "update_proportion = 0.25\n",
    "\n",
    "extrinsic_advantage_coef = 2\n",
    "intrinsic_advantage_coef = 1\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self,width=240,height=256,channel = 3,action_dim=7,learning_rate=0.0005):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.channel = channel\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        super(Agent,self).__init__()\n",
    "        \n",
    "        self.memory = [[] for _ in range(env_num)]\n",
    "        self.intrinsic_queue_list = [queue.Queue() for _ in range(env_num)]\n",
    "        self.intrinsic_input_queue_list = [queue.Queue() for _ in range(env_num)]\n",
    "        self.ppo = PPO(self.width, self.height, self.channel, self.action_dim)\n",
    "        self.rnd = RND(self.width, self.height , self.channel)\n",
    "        self.optimizer = optim.Adam(self.parameters(),lr = learning_rate)\n",
    "    def put_data(self,i,data):\n",
    "        self.memory[i].append(data)\n",
    "    def make_batch(self,i):\n",
    "        state_list, action_list, extrinsic_reward_list, intrinsic_reward_list, next_state_list, \\\n",
    "        prob_list, extrinsic_done_list,intrinsic_done_list = [],[],[],[],[],[], [],[]\n",
    "        for data in self.memory[i]:\n",
    "            state,action,extrinsic_reward, intrinsic_reward,next_state,prob,done = data\n",
    "            state_list.append(state)\n",
    "            action_list.append([action])\n",
    "            extrinsic_reward_list.append([extrinsic_reward])\n",
    "            intrinsic_reward_list.append([intrinsic_reward])\n",
    "\n",
    "            next_state_list.append(next_state)\n",
    "            extrinsic_done_mask = 0 if done else 1\n",
    "            extrinsic_done_list.append([extrinsic_done_mask])\n",
    "            intrinsic_done_list.append([1])\n",
    "            prob_list.append([prob])\n",
    "        self.memory[i] = []\n",
    "\n",
    "        s,a,er,ir,next_s,extrinsic_done_list,intrinsic_done_list,prob \\\n",
    "                                        = torch.tensor(state_list,dtype=torch.float),\\\n",
    "                                        torch.tensor(action_list),\\\n",
    "                                        torch.tensor(extrinsic_reward_list,dtype = torch.float),\\\n",
    "                                        torch.tensor(intrinsic_reward_list,dtype = torch.float),\\\n",
    "                                        torch.tensor(next_state_list,dtype=torch.float),\\\n",
    "                                        torch.tensor(extrinsic_done_list,dtype = torch.float),\\\n",
    "                                        torch.tensor(intrinsic_done_list,dtype = torch.float),\\\n",
    "                                        torch.tensor(prob_list,dtype = torch.float)\n",
    "        if gpu:\n",
    "            return s.cuda(),a.cuda(),er.cuda(),ir.cuda(),next_s.cuda(),extrinsic_done_list.cuda()\\\n",
    "            ,intrinsic_done_list.cuda(),prob.cuda() \n",
    "        else :\n",
    "            return s,a,er,ir,next_s,extrinsic_done_list,intrinsic_done_list,prob  \n",
    "    \n",
    "    def train(self,i):\n",
    "        state,action,extrinsic_reward,intrinsic_reward, next_state,extrinsic_done_list,\\\n",
    "        intrinsic_done_list,action_prob = self.make_batch(i)\n",
    "        \n",
    "        for k in range(K_epoch):\n",
    "            \n",
    "\n",
    "            state = state.squeeze()\n",
    "            next_state = next_state.squeeze()\n",
    "            predicted_action, predicted_extrinsic, predicted_intrinsic = self.ppo(state)\n",
    "            predicted_next_action, predicted_next_extrinsic, predicted_next_intrinsic = self.ppo(next_state)\n",
    "\n",
    "            if gpu:\n",
    "                intrinsic_next_state_mean = torch.mean(torch.cat(list(self.intrinsic_input_queue_list[i].queue)),dim = 0).cuda()\n",
    "                #if len(model.trinsic_input_queue_list[i].queue) == 1:\n",
    "                    \n",
    "                intrinsic_next_state_std = torch.std(torch.cat(list(model.intrinsic_input_queue_list[i].queue)),dim = 0).cuda()\n",
    "            \n",
    "                preprocessed_next_state = torch.clamp(((next_state - intrinsic_next_state_mean) / \\\n",
    "                                                   (intrinsic_next_state_std + torch.tensor(1e-8).cuda())),-5,5)\n",
    "            else:\n",
    "                intrinsic_next_state_mean = torch.mean(torch.cat(list(self.intrinsic_input_queue_list[i].queue)),dim = 0)\n",
    "                intrinsic_next_state_std = torch.std(torch.cat(list(model.intrinsic_input_queue_list[i].queue)),dim = 0)\n",
    "            \n",
    "                preprocessed_next_state = torch.clamp(((next_state - intrinsic_next_state_mean) / \\\n",
    "                                                   (intrinsic_next_state_std + torch.tensor(1e-8))),-5,5)\n",
    "            \n",
    "            predict_feature, target_feature = self.rnd(preprocessed_next_state)\n",
    "            td_error = extrinsic_reward + extrinsic_gamma * predicted_next_extrinsic * extrinsic_done_list\n",
    "            delta = td_error - predicted_next_extrinsic\n",
    "            if gpu:\n",
    "                delta = delta.detach().cpu().numpy()\n",
    "            else:\n",
    "                delta = delta.detach().numpy()\n",
    "            advantage_list = []\n",
    "            \n",
    "            advantage = 0.0\n",
    "            for delta_t in delta[::-1]:\n",
    "                advantage = gamma * lmbda * advantage + delta_t[0]\n",
    "                advantage_list.append([advantage])\n",
    "            advantage_list.reverse()\n",
    "            if gpu:\n",
    "                advantage = torch.tensor(advantage_list,dtype = torch.float).cuda()\n",
    "            else:\n",
    "                advantage = torch.tensor(advantage_list,dtype = torch.float)\n",
    "            ##intrinsic_advantage\n",
    "            intrinsic_td_error = intrinsic_reward + intrinsic_gamma * predicted_next_intrinsic * intrinsic_done_list\n",
    "            intrinsic_delta = intrinsic_td_error - predicted_next_intrinsic\n",
    "            if gpu:\n",
    "                intrinsic_delta = intrinsic_delta.detach().cpu().numpy()\n",
    "            else:\n",
    "                intrinsic_delta = intrinsic_delta.detach().numpy()\n",
    "            \n",
    "\n",
    "            \n",
    "            intrinsic_advantage_list = []\n",
    "            intrinsic_advantage = 0.0\n",
    "            \n",
    "            for intrinsic_delta_t in intrinsic_delta[::-1]:\n",
    "                intrinsic_advantage = gamma * lmbda * intrinsic_advantage + intrinsic_delta_t[0]\n",
    "                intrinsic_advantage_list.append([intrinsic_advantage])\n",
    "            intrinsic_advantage_list.reverse()\n",
    "            if gpu:\n",
    "                intrinsic_advantage = torch.tensor(intrinsic_advantage_list,dtype = torch.float).cuda()\n",
    "            else:\n",
    "                intrinsic_advantage = torch.tensor(intrinsic_advantage_list,dtype = torch.float)\n",
    "            #### intrinsic_error\n",
    "            intrinsic_error = (predict_feature - target_feature.detach()).pow(2).sum(0)\n",
    "            if gpu:\n",
    "                masking = torch.rand(len(intrinsic_error)).cuda()\n",
    "                masking = (masking < update_proportion).type(torch.FloatTensor).cuda()\n",
    "            else:\n",
    "                masking = torch.rand(len(intrinsic_error))\n",
    "                masking = (masking < update_proportion).type(torch.FloatTensor)\n",
    "            if gpu:\n",
    "                intrinsic_error = (intrinsic_error * masking).sum() / torch.max(masking.sum(), torch.Tensor([1]).cuda())\n",
    "            else:\n",
    "                intrinsic_error = (intrinsic_error * masking).sum() / torch.max(masking.sum(), torch.Tensor([1]))\n",
    "\n",
    "            \n",
    "            now_action = predicted_action\n",
    "            m = Categorical(now_action)\n",
    "            entropy = m.entropy().mean()\n",
    "            \n",
    "            \n",
    "            now_action = now_action.gather(1,action)\n",
    "            \n",
    "            \n",
    "            ratio = torch.exp(torch.log(now_action) - torch.log(action_prob))\n",
    "            advantage = extrinsic_advantage_coef * advantage +  intrinsic_advantage_coef * intrinsic_advantage\n",
    "            surr1 = ratio * advantage\n",
    "            surr2 = torch.clamp(ratio , 1-eps_clip, 1 + eps_clip) * advantage\n",
    "            loss = - torch.min(surr1,surr2).mean() + critic_coef * (F.smooth_l1_loss(predicted_extrinsic,td_error.detach()) +\\\n",
    "                    intrinsic_error) - ent_coef * entropy.mean()  #+ F.mse_loss(predict_feature, target_feature)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "env_num = 4\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "\n",
    "env_list = [gym_super_mario_bros.make('SuperMarioBros-1-1-v0'),\\\n",
    "           gym_super_mario_bros.make('SuperMarioBros-1-1-v0'),\\\n",
    "           gym_super_mario_bros.make('SuperMarioBros-1-1-v0'),\\\n",
    "           gym_super_mario_bros.make('SuperMarioBros-1-1-v0')]\n",
    "env_list = [JoypadSpace(env, SIMPLE_MOVEMENT) for env in env_list]\n",
    "\n",
    "#env_list = [gym_super_mario_bros.make('SuperMarioBros-1-'+str(i)+'-v0') for i in range(1,env_num+1)]\n",
    "#env_list = [JoypadSpace(env, SIMPLE_MOVEMENT) for env in env_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if gpu:\n",
    "    model = Agent().cuda()\n",
    "else:\n",
    "    model = Agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_horizon_list = [32,64,96,128]\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    #clear_output()\n",
    "    if epoch > 0:\n",
    "        print('epoch : ',epoch, 'info_list : ',info_list)\n",
    "    global_step_list = [0 for _ in range(env_num)]\n",
    "    model.intrinsic_queue = [queue.Queue() for _ in range(env_num)]\n",
    "    model.intrinsic_input_queue = [queue.Queue() for _ in range(env_num)]\n",
    "    state_list = [env.reset() for env in env_list]\n",
    "    state_list = [np.array(state)/255 for state in state_list]\n",
    "            #state = np.transpose(state,(2,0,1))\n",
    "    state_list = [np.moveaxis(state, -1, 0) for state in state_list]\n",
    "    state_list = [torch.tensor(state).float() for state in state_list]\n",
    "    state_list = [state.unsqueeze(0) for state in state_list]\n",
    "    done_list = [False for _ in range(env_num)]\n",
    "    \n",
    "    action_prob_list = [[] for _ in range(env_num)]\n",
    "    m_list = [[] for _ in range(env_num)]\n",
    "    action_list = [[] for _ in range(env_num)]\n",
    "    next_state_list = [[] for _ in range(env_num)]\n",
    "    extrinsic_reward_list = [[] for _ in range(env_num)]\n",
    "    info_list = [[] for _ in range(env_num)]\n",
    "    next_state_list = [[] for _ in range(env_num)]\n",
    "    \n",
    "    while not all(done_list) :\n",
    "        for i in range(env_num):\n",
    "            if not done_list[i] :\n",
    "                for t in range(T_horizon_list[i]):\n",
    "                    #env.render()\n",
    "                    global_step_list[i] +=1\n",
    "\n",
    "                    if gpu:\n",
    "                        action_prob_list[i], _ , _ = model.ppo.forward(state_list[i].cuda())\n",
    "                    else:\n",
    "                        action_prob_list[i], _ , _ = model.ppo.forward(state_list[i])\n",
    "                    m_list[i] = Categorical(action_prob_list[i])\n",
    "\n",
    "                    action_list[i] = m_list[i].sample().item()\n",
    "\n",
    "\n",
    "                    next_state, extrinsic_reward_list[i], done_list[i], info_list[i] = env_list[i].step(action_list[i])\n",
    "                    next_state = np.array(next_state)/255\n",
    "                    next_state = np.moveaxis(next_state,-1,0)\n",
    "                    next_state = torch.tensor(next_state).float()\n",
    "                    next_state = next_state.unsqueeze(0)\n",
    "                    next_state_list[i] = next_state\n",
    "\n",
    "                    model.intrinsic_input_queue_list[i].put(next_state)\n",
    "                    if len(model.intrinsic_input_queue_list[i].queue) > 128:\n",
    "                        model.intrinsic_input_queue_list[i].get()\n",
    "                    intrinsic_next_state_mean = \\\n",
    "                            torch.mean(torch.cat(list(model.intrinsic_input_queue_list[i].queue)),dim = 0)\n",
    "                    if len(model.intrinsic_input_queue_list[i].queue) == 1:\n",
    "                        intrinsic_next_state_std = torch.zeros(1)\n",
    "                    else:\n",
    "                        intrinsic_next_state_std = \\\n",
    "                                torch.std(torch.cat(list(model.intrinsic_input_queue_list[i].queue)),dim = 0)\n",
    "\n",
    "                    preprocessed_next_state = \\\n",
    "                            torch.clamp(((next_state - intrinsic_next_state_mean) / (intrinsic_next_state_std + 1e-8)), -5,5)\n",
    "\n",
    "\n",
    "                    #(model.intrinsic_input_queue)\n",
    "                    if gpu:\n",
    "                        predictor,target = model.rnd.forward(preprocessed_next_state.cuda())\n",
    "                    else:\n",
    "                        predictor,target = model.rnd.forward(preprocessed_next_state)\n",
    "                    intrinsic_reward = (predictor - target).pow(2).sum(1) / 2\n",
    "                    if len(model.intrinsic_queue_list[i].queue) > 128:\n",
    "                        model.intrinsic_queue_list[i].get()\n",
    "                    model.intrinsic_queue_list[i].put(intrinsic_reward.item())\n",
    "                    intrinsic_mean = np.mean(model.intrinsic_queue_list[i].queue)\n",
    "                    if len(model.intrinsic_queue_list[i].queue) == 1:\n",
    "                        if gpu:\n",
    "                            intrinsic_std = torch.zeros(1).cuda()\n",
    "                        else:\n",
    "                            intrinsic_std = torch.zeros(1)\n",
    "                    else:\n",
    "                        intrinsic_std = np.std(model.intrinsic_queue_list[i].queue)\n",
    "                    \n",
    "                    intrinsic_reward = (intrinsic_reward - intrinsic_mean) / (intrinsic_std+ 1e-8)\n",
    "\n",
    "                    if (info_list[i]['time'] == 0)  or(global_step_list[i] > 1000):\n",
    "                        done_list[i] = True\n",
    "                        reward = -10.\n",
    "\n",
    "                        \n",
    "                        \n",
    "                    model.put_data(i,((state_list[i].tolist(), \\\n",
    "                                    action_list[i], extrinsic_reward_list[i]/100,\\\n",
    "                                    (intrinsic_reward.item()/100), next_state.tolist(), \\\n",
    "                                    action_prob_list[i][0][action_list[i]].item(), done_list[i])))\n",
    "                    #print('env number : ',i,', global_step : ',global_step_list[i],', action : ', action_list[i],'action_prob : ',action_prob_list[i].tolist()[0])\n",
    "                    #print('extrinsic_reward : ',extrinsic_reward_list[i],'intrinsic_reward : ',intrinsic_reward.item())\n",
    "                    #print('place',info_list[i]['x_pos'])\n",
    "                    #print('info',info_list[i])\n",
    "                    if done_list[i] :\n",
    "                        print('env_num : ',i', epoch : ',epoch, ', global_step : ',global_step_list[i])\n",
    "                        break\n",
    "                    state_list[i] = next_state_list[i]\n",
    "        for i in range(env_num) :\n",
    "            if len(model.memory[i]) > 1:\n",
    "                model.train(i)\n",
    "    #torch.save(model.state_dict(), 'gdrive/My Drive/supermario_checkpoint/'+str(epoch))\n",
    "    #env.render()\n",
    "\n",
    "#env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unity",
   "language": "python",
   "name": "unity"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
