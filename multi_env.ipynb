{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from torch.nn import init\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "import queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RND(nn.Module):\n",
    "    def __init__(self,width = 240, height =256, channel = 3):\n",
    "        super(RND,self).__init__()\n",
    "        \n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.channel = channel\n",
    "        \n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=3,\n",
    "                out_channels=32,\n",
    "                kernel_size=8,\n",
    "                stride=4),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(\n",
    "                in_channels=32,\n",
    "                out_channels=64,\n",
    "                kernel_size=4,\n",
    "                stride=2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(\n",
    "                in_channels=64,\n",
    "                out_channels=64,\n",
    "                kernel_size=3,\n",
    "                stride=1),\n",
    "            nn.LeakyReLU(),\n",
    "            Flatten(),\n",
    "            nn.Linear(46592, 512), # change\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512)\n",
    "            )\n",
    "        self.target = copy.deepcopy(self.predictor)\n",
    "        for p in self.modules():\n",
    "            if isinstance(p, nn.Conv2d):\n",
    "                init.orthogonal_(p.weight, np.sqrt(2))\n",
    "                p.bias.data.zero_()\n",
    "\n",
    "            if isinstance(p, nn.Linear):\n",
    "                init.orthogonal_(p.weight, np.sqrt(2))\n",
    "                p.bias.data.zero_()\n",
    "\n",
    "        for param in self.target.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "    def forward(self, state):\n",
    "        target_feature = self.target(state)\n",
    "        predict_feature = self.predictor(state)\n",
    "        return predict_feature, target_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "class PPO(nn.Module):\n",
    "    def __init__(self,width = 240, height =256, channel = 3, action_dim = 7):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.channel = channel\n",
    "        self.action_dim = action_dim\n",
    "        super(PPO,self).__init__()\n",
    "\n",
    "        \n",
    "        self.basic = nn.Sequential(\\\n",
    "                    nn.Conv2d(in_channels = 3,\\\n",
    "                             out_channels = 32,\\\n",
    "                             kernel_size = 8,\\\n",
    "                              stride = 4),\n",
    "                    nn.ReLU(),\n",
    "                                   nn.Conv2d(in_channels = 32,\\\n",
    "                                           out_channels = 64,\\\n",
    "                                           kernel_size = 4,\\\n",
    "                                           stride = 2),\\\n",
    "                    nn.ReLU(),\\\n",
    "                    nn.Conv2d(in_channels = 64,\\\n",
    "                             out_channels = 64,\\\n",
    "                             kernel_size = 3,\\\n",
    "                             stride = 1),\n",
    "                                   nn.ReLU(),\\\n",
    "                                   Flatten(),\n",
    "                    nn.Linear(46592,256), #have to change       \n",
    "                    nn.ReLU(),\\\n",
    "                    nn.Linear(256,448),\n",
    "                    nn.ReLU()\n",
    "                    )\n",
    "        self.actor = nn.Sequential(\\\n",
    "                                  nn.Linear(448,448),\\\n",
    "                                  nn.ReLU(),\\\n",
    "                                  nn.Linear(448,self.action_dim)\\\n",
    "                                  )\n",
    "        \n",
    "        self.extrinsic_critic = nn.Linear(448,1)\n",
    "        self.intrinsic_critic = nn.Linear(448,1)\n",
    "        \n",
    "        init.orthogonal_(self.extrinsic_critic.weight, 0.01)\n",
    "        self.extrinsic_critic.bias.data.zero_()\n",
    "\n",
    "        init.orthogonal_(self.intrinsic_critic.weight, 0.01)\n",
    "        self.intrinsic_critic.bias.data.zero_()\n",
    "\n",
    "        for i in range(len(self.actor)):\n",
    "            if type(self.actor[i]) == nn.Linear:\n",
    "                init.orthogonal_(self.actor[i].weight, 0.01)\n",
    "                self.actor[i].bias.data.zero_()\n",
    "    def forward(self, x,dim = -1):\n",
    "        x = self.basic(x)\n",
    "        action = self.actor(x)\n",
    "        action_prob = F.softmax(action,dim = dim)\n",
    "        \n",
    "        intrinsic = self.intrinsic_critic(x)\n",
    "        extrinsic = self.extrinsic_critic(x)\n",
    "        return action_prob,extrinsic,intrinsic    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "gamma         = 0.98\n",
    "lmbda         = 0.95\n",
    "eps_clip      = 0.1\n",
    "K_epoch       = 4\n",
    "T_horizon     = 128\n",
    "critic_coef = 0.5\n",
    "ent_coef = 0.001\n",
    "intrinsic_gamma = 0.99\n",
    "extrinsic_gamma = 0.999\n",
    "update_proportion = 0.25\n",
    "\n",
    "extrinsic_advantage_coef = 2\n",
    "intrinsic_advantage_coef = 1\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self,width=240,height=256,channel = 3,action_dim=7,learning_rate=0.0005):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.channel = channel\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        super(Agent,self).__init__()\n",
    "        \n",
    "        self.memory = [[] for _ in range(env_num)]\n",
    "        self.intrinsic_queue_list = [queue.Queue() for _ in range(env_num)]\n",
    "        self.intrinsic_input_queue_list = [queue.Queue() for _ in range(env_num)]\n",
    "        self.ppo = PPO(self.width, self.height, self.channel, self.action_dim)\n",
    "        self.rnd = RND(self.width, self.height , self.channel)\n",
    "        self.optimizer = optim.Adam(self.parameters(),lr = learning_rate)\n",
    "    def put_data(self,i,data):\n",
    "        self.memory[i].append(data)\n",
    "    def make_batch(self,i):\n",
    "        state_list, action_list, extrinsic_reward_list, intrinsic_reward_list, next_state_list, \\\n",
    "        prob_list, extrinsic_done_list,intrinsic_done_list = [],[],[],[],[],[], [],[]\n",
    "        for data in self.memory[i]:\n",
    "            state,action,extrinsic_reward, intrinsic_reward,next_state,prob,done = data\n",
    "            state_list.append(state)\n",
    "            action_list.append([action])\n",
    "            extrinsic_reward_list.append([extrinsic_reward])\n",
    "            intrinsic_reward_list.append([intrinsic_reward])\n",
    "\n",
    "            next_state_list.append(next_state)\n",
    "            extrinsic_done_mask = 0 if done else 1\n",
    "            extrinsic_done_list.append([extrinsic_done_mask])\n",
    "            intrinsic_done_list.append([1])\n",
    "            prob_list.append([prob])\n",
    "        self.memory[i] = []\n",
    "\n",
    "        s,a,er,ir,next_s,extrinsic_done_list,intrinsic_done_list,prob \\\n",
    "                                        = torch.tensor(state_list,dtype=torch.float),\\\n",
    "                                        torch.tensor(action_list),\\\n",
    "                                        torch.tensor(extrinsic_reward_list,dtype = torch.float),\\\n",
    "                                        torch.tensor(intrinsic_reward_list,dtype = torch.float),\\\n",
    "                                        torch.tensor(next_state_list,dtype=torch.float),\\\n",
    "                                        torch.tensor(extrinsic_done_list,dtype = torch.float),\\\n",
    "                                        torch.tensor(intrinsic_done_list,dtype = torch.float),\\\n",
    "                                        torch.tensor(prob_list,dtype = torch.float)\n",
    "        if gpu:\n",
    "            return s.cuda(),a.cuda(),er.cuda(),ir.cuda(),next_s.cuda(),extrinsic_done_list.cuda()\\\n",
    "            ,intrinsic_done_list.cuda(),prob.cuda() \n",
    "        else :\n",
    "            return s,a,er,ir,next_s,extrinsic_done_list,intrinsic_done_list,prob  \n",
    "    \n",
    "    def train(self,i):\n",
    "        state,action,extrinsic_reward,intrinsic_reward, next_state,extrinsic_done_list,\\\n",
    "        intrinsic_done_list,action_prob = self.make_batch(i)\n",
    "        \n",
    "        for k in range(K_epoch):\n",
    "            global intrinsic_error\n",
    "            global masking\n",
    "            global predict_feature\n",
    "            global  target_feature\n",
    "            \n",
    "\n",
    "            state = state.squeeze()\n",
    "            next_state = next_state.squeeze()\n",
    "            predicted_action, predicted_extrinsic, predicted_intrinsic = self.ppo(state)\n",
    "            predicted_next_action, predicted_next_extrinsic, predicted_next_intrinsic = self.ppo(next_state)\n",
    "\n",
    "            if gpu:\n",
    "                intrinsic_next_state_mean = torch.mean(torch.cat(list(self.intrinsic_input_queue_list[i].queue)),dim = 0).cuda()\n",
    "                #if len(model.trinsic_input_queue_list[i].queue) == 1:\n",
    "                    \n",
    "                intrinsic_next_state_std = torch.std(torch.cat(list(model.intrinsic_input_queue_list[i].queue)),dim = 0).cuda()\n",
    "            \n",
    "                preprocessed_next_state = torch.clamp(((next_state - intrinsic_next_state_mean) / \\\n",
    "                                                   (intrinsic_next_state_std + torch.tensor(1e-8).cuda())),-5,5)\n",
    "            else:\n",
    "                intrinsic_next_state_mean = torch.mean(torch.cat(list(self.intrinsic_input_queue_list[i].queue)),dim = 0)\n",
    "                intrinsic_next_state_std = torch.std(torch.cat(list(model.intrinsic_input_queue_list[i].queue)),dim = 0)\n",
    "            \n",
    "                preprocessed_next_state = torch.clamp(((next_state - intrinsic_next_state_mean) / \\\n",
    "                                                   (intrinsic_next_state_std + torch.tensor(1e-8))),-5,5)\n",
    "            \n",
    "            predict_feature, target_feature = self.rnd(preprocessed_next_state)\n",
    "            td_error = extrinsic_reward + extrinsic_gamma * predicted_next_extrinsic * extrinsic_done_list\n",
    "            delta = td_error - predicted_next_extrinsic\n",
    "            if gpu:\n",
    "                delta = delta.detach().cpu().numpy()\n",
    "            else:\n",
    "                delta = delta.detach().numpy()\n",
    "            advantage_list = []\n",
    "            \n",
    "            advantage = 0.0\n",
    "            for delta_t in delta[::-1]:\n",
    "                advantage = gamma * lmbda * advantage + delta_t[0]\n",
    "                advantage_list.append([advantage])\n",
    "            advantage_list.reverse()\n",
    "            if gpu:\n",
    "                advantage = torch.tensor(advantage_list,dtype = torch.float).cuda()\n",
    "            else:\n",
    "                advantage = torch.tensor(advantage_list,dtype = torch.float)\n",
    "            ##intrinsic_advantage\n",
    "            intrinsic_td_error = intrinsic_reward + intrinsic_gamma * predicted_next_intrinsic * intrinsic_done_list\n",
    "            intrinsic_delta = intrinsic_td_error - predicted_next_intrinsic\n",
    "            if gpu:\n",
    "                intrinsic_delta = intrinsic_delta.detach().cpu().numpy()\n",
    "            else:\n",
    "                intrinsic_delta = intrinsic_delta.detach().numpy()\n",
    "            \n",
    "\n",
    "            \n",
    "            intrinsic_advantage_list = []\n",
    "            intrinsic_advantage = 0.0\n",
    "            \n",
    "            for intrinsic_delta_t in intrinsic_delta[::-1]:\n",
    "                intrinsic_advantage = gamma * lmbda * intrinsic_advantage + intrinsic_delta_t[0]\n",
    "                intrinsic_advantage_list.append([intrinsic_advantage])\n",
    "            intrinsic_advantage_list.reverse()\n",
    "            if gpu:\n",
    "                intrinsic_advantage = torch.tensor(intrinsic_advantage_list,dtype = torch.float).cuda()\n",
    "            else:\n",
    "                intrinsic_advantage = torch.tensor(intrinsic_advantage_list,dtype = torch.float)\n",
    "            #### intrinsic_error\n",
    "            intrinsic_error = (predict_feature - target_feature.detach()).pow(2).sum(0)\n",
    "            if gpu:\n",
    "                masking = torch.rand(len(intrinsic_error)).cuda()\n",
    "                masking = (masking < update_proportion).type(torch.FloatTensor).cuda()\n",
    "            else:\n",
    "                masking = torch.rand(len(intrinsic_error))\n",
    "                masking = (masking < update_proportion).type(torch.FloatTensor)\n",
    "            if gpu:\n",
    "                intrinsic_error = (intrinsic_error * masking).sum() / torch.max(masking.sum(), torch.Tensor([1]).cuda())\n",
    "            else:\n",
    "                intrinsic_error = (intrinsic_error * masking).sum() / torch.max(masking.sum(), torch.Tensor([1]))\n",
    "            #raise Exception() #asdasd\n",
    "            \n",
    "            now_action = predicted_action\n",
    "            m = Categorical(now_action)\n",
    "            entropy = m.entropy().mean()\n",
    "            \n",
    "            \n",
    "            now_action = now_action.gather(1,action)\n",
    "            \n",
    "            \n",
    "            ratio = torch.exp(torch.log(now_action) - torch.log(action_prob))\n",
    "            advantage = extrinsic_advantage_coef * advantage +  intrinsic_advantage_coef * intrinsic_advantage\n",
    "            surr1 = ratio * advantage\n",
    "            surr2 = torch.clamp(ratio , 1-eps_clip, 1 + eps_clip) * advantage\n",
    "            loss = - torch.min(surr1,surr2) + critic_coef * (F.smooth_l1_loss(predicted_extrinsic,td_error.detach()) +\\\n",
    "                    intrinsic_error) - ent_coef * entropy  #+ F.mse_loss(predict_feature, target_feature)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "env_num = 4\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "\n",
    "env_list = [gym_super_mario_bros.make('SuperMarioBros-v0') for _ in range(env_num)]\n",
    "env_list = [JoypadSpace(env, SIMPLE_MOVEMENT) for env in env_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['NOOP'],\n",
       " ['right'],\n",
       " ['right', 'A'],\n",
       " ['right', 'B'],\n",
       " ['right', 'A', 'B'],\n",
       " ['A'],\n",
       " ['left']]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "if gpu:\n",
    "    model = Agent().cuda()\n",
    "else:\n",
    "    model = Agent()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env number :  0 , global_step :  1 , action :  0 action_prob :  [0.14285697042942047, 0.14285726845264435, 0.14285710453987122, 0.14285655319690704, 0.1428573578596115, 0.14285773038864136, 0.14285700023174286]\n",
      "extrinsic_reward :  0 intrinsic_reward :  0.0\n",
      "place 40\n",
      "env number :  0 , global_step :  2 , action :  4 action_prob :  [0.14285697042942047, 0.14285726845264435, 0.14285710453987122, 0.14285655319690704, 0.1428573578596115, 0.14285773038864136, 0.14285700023174286]\n",
      "extrinsic_reward :  0 intrinsic_reward :  0.0\n",
      "place 40\n",
      "env number :  0 , global_step :  3 , action :  6 action_prob :  [0.14285697042942047, 0.14285726845264435, 0.14285710453987122, 0.14285655319690704, 0.1428573578596115, 0.14285773038864136, 0.14285700023174286]\n",
      "extrinsic_reward :  0 intrinsic_reward :  0.0\n",
      "place 40\n",
      "env number :  0 , global_step :  4 , action :  5 action_prob :  [0.14285697042942047, 0.14285726845264435, 0.14285710453987122, 0.14285655319690704, 0.1428573578596115, 0.14285773038864136, 0.14285700023174286]\n",
      "extrinsic_reward :  0 intrinsic_reward :  1.732050895690918\n",
      "place 40\n",
      "env number :  0 , global_step :  5 , action :  6 action_prob :  [0.14285697042942047, 0.14285726845264435, 0.14285710453987122, 0.14285655319690704, 0.14285734295845032, 0.14285773038864136, 0.14285700023174286]\n",
      "extrinsic_reward :  0 intrinsic_reward :  1.9999818801879883\n",
      "place 40\n",
      "env number :  1 , global_step :  1 , action :  4 action_prob :  [0.14285697042942047, 0.14285726845264435, 0.14285710453987122, 0.14285655319690704, 0.1428573578596115, 0.14285773038864136, 0.14285700023174286]\n",
      "extrinsic_reward :  0 intrinsic_reward :  0.0\n",
      "place 40\n",
      "env number :  1 , global_step :  2 , action :  3 action_prob :  [0.14285697042942047, 0.14285726845264435, 0.14285710453987122, 0.14285655319690704, 0.1428573578596115, 0.14285773038864136, 0.14285700023174286]\n",
      "extrinsic_reward :  0 intrinsic_reward :  0.0\n",
      "place 40\n",
      "env number :  1 , global_step :  3 , action :  3 action_prob :  [0.14285697042942047, 0.14285726845264435, 0.14285710453987122, 0.14285655319690704, 0.1428573578596115, 0.14285773038864136, 0.14285700023174286]\n",
      "extrinsic_reward :  0 intrinsic_reward :  1.4142134189605713\n",
      "place 40\n",
      "env number :  1 , global_step :  4 , action :  3 action_prob :  [0.14285697042942047, 0.14285726845264435, 0.14285710453987122, 0.14285655319690704, 0.14285734295845032, 0.14285773038864136, 0.14285700023174286]\n",
      "extrinsic_reward :  0 intrinsic_reward :  1.1927746534347534\n",
      "place 40\n",
      "env number :  1 , global_step :  5 , action :  3 action_prob :  [0.14285698533058167, 0.14285726845264435, 0.14285710453987122, 0.14285655319690704, 0.14285734295845032, 0.14285773038864136, 0.14285698533058167]\n",
      "extrinsic_reward :  0 intrinsic_reward :  1.9999892711639404\n",
      "place 40\n",
      "env number :  2 , global_step :  1 , action :  2 action_prob :  [0.14285697042942047, 0.14285726845264435, 0.14285710453987122, 0.14285655319690704, 0.1428573578596115, 0.14285773038864136, 0.14285700023174286]\n",
      "extrinsic_reward :  0 intrinsic_reward :  0.0\n",
      "place 40\n",
      "env number :  2 , global_step :  2 , action :  1 action_prob :  [0.14285697042942047, 0.14285726845264435, 0.14285710453987122, 0.14285655319690704, 0.1428573578596115, 0.14285773038864136, 0.14285700023174286]\n",
      "extrinsic_reward :  0 intrinsic_reward :  0.0\n",
      "place 40\n",
      "env number :  2 , global_step :  3 , action :  3 action_prob :  [0.14285697042942047, 0.14285726845264435, 0.14285710453987122, 0.14285655319690704, 0.1428573578596115, 0.14285773038864136, 0.14285700023174286]\n",
      "extrinsic_reward :  0 intrinsic_reward :  1.4142134189605713\n",
      "place 40\n",
      "env number :  2 , global_step :  4 , action :  2 action_prob :  [0.14285697042942047, 0.14285726845264435, 0.14285710453987122, 0.14285655319690704, 0.14285734295845032, 0.14285773038864136, 0.14285700023174286]\n",
      "extrinsic_reward :  0 intrinsic_reward :  1.1927746534347534\n",
      "place 40\n",
      "env number :  2 , global_step :  5 , action :  3 action_prob :  [0.14285698533058167, 0.14285726845264435, 0.14285710453987122, 0.14285655319690704, 0.14285734295845032, 0.14285773038864136, 0.14285698533058167]\n",
      "extrinsic_reward :  0 intrinsic_reward :  1.9999892711639404\n",
      "place 40\n",
      "env number :  3 , global_step :  1 , action :  1 action_prob :  [0.14285697042942047, 0.14285726845264435, 0.14285710453987122, 0.14285655319690704, 0.1428573578596115, 0.14285773038864136, 0.14285700023174286]\n",
      "extrinsic_reward :  0 intrinsic_reward :  0.0\n",
      "place 40\n",
      "env number :  3 , global_step :  2 , action :  0 action_prob :  [0.14285697042942047, 0.14285726845264435, 0.14285710453987122, 0.14285655319690704, 0.1428573578596115, 0.14285773038864136, 0.14285700023174286]\n",
      "extrinsic_reward :  0 intrinsic_reward :  0.0\n",
      "place 40\n",
      "env number :  3 , global_step :  3 , action :  1 action_prob :  [0.14285697042942047, 0.14285726845264435, 0.14285710453987122, 0.14285655319690704, 0.1428573578596115, 0.14285773038864136, 0.14285700023174286]\n",
      "extrinsic_reward :  0 intrinsic_reward :  1.4142135381698608\n",
      "place 40\n",
      "env number :  3 , global_step :  4 , action :  4 action_prob :  [0.14285698533058167, 0.14285726845264435, 0.14285710453987122, 0.14285655319690704, 0.1428573578596115, 0.14285773038864136, 0.14285700023174286]\n",
      "extrinsic_reward :  0 intrinsic_reward :  0.8452314734458923\n",
      "place 40\n",
      "env number :  3 , global_step :  5 , action :  4 action_prob :  [0.14285697042942047, 0.14285726845264435, 0.14285710453987122, 0.14285655319690704, 0.14285734295845032, 0.14285773038864136, 0.14285700023174286]\n",
      "extrinsic_reward :  0 intrinsic_reward :  1.9999985694885254\n",
      "place 40\n",
      "env number :  0 , global_step :  6 , action :  4 action_prob :  [0.16800561547279358, 0.16144558787345886, 0.17729486525058746, 0.11298346519470215, 0.16220766305923462, 0.10955122113227844, 0.10851158946752548]\n",
      "extrinsic_reward :  0 intrinsic_reward :  -0.44382837414741516\n",
      "place 40\n",
      "env number :  0 , global_step :  7 , action :  1 action_prob :  [0.16800275444984436, 0.16144351661205292, 0.1772906333208084, 0.1129869669675827, 0.16220560669898987, 0.10955499857664108, 0.10851547867059708]\n",
      "extrinsic_reward :  0 intrinsic_reward :  2.449281692504883\n",
      "place 40\n",
      "env number :  0 , global_step :  8 , action :  1 action_prob :  [0.16799890995025635, 0.16144074499607086, 0.17728491127490997, 0.11299169808626175, 0.16220282018184662, 0.10956013947725296, 0.10852076858282089]\n",
      "extrinsic_reward :  0 intrinsic_reward :  1.7322014570236206\n",
      "place 40\n",
      "env number :  0 , global_step :  9 , action :  5 action_prob :  [0.1679922342300415, 0.16143593192100525, 0.17727500200271606, 0.11299990117549896, 0.162198007106781, 0.1095690056681633, 0.10852991789579391]\n",
      "extrinsic_reward :  0 intrinsic_reward :  1.3707345724105835\n",
      "place 40\n",
      "env number :  0 , global_step :  10 , action :  2 action_prob :  [0.16798806190490723, 0.1614329218864441, 0.17726881802082062, 0.11300500482320786, 0.16219499707221985, 0.10957453399896622, 0.10853561013936996]\n",
      "extrinsic_reward :  0 intrinsic_reward :  2.3939595222473145\n",
      "place 40\n",
      "env number :  1 , global_step :  6 , action :  2 action_prob :  [0.16800275444984436, 0.16144351661205292, 0.1772906333208084, 0.1129869669675827, 0.16220560669898987, 0.10955499857664108, 0.10851547867059708]\n",
      "extrinsic_reward :  1 intrinsic_reward :  -0.44518229365348816\n",
      "place 41\n",
      "env number :  1 , global_step :  7 , action :  0 action_prob :  [0.16799890995025635, 0.16144074499607086, 0.17728491127490997, 0.11299169808626175, 0.16220282018184662, 0.10956013947725296, 0.10852076858282089]\n",
      "extrinsic_reward :  0 intrinsic_reward :  2.4492809772491455\n",
      "place 41\n",
      "env number :  1 , global_step :  8 , action :  5 action_prob :  [0.16799069941043854, 0.1614348143339157, 0.1772727072238922, 0.11300178617238998, 0.16219688951969147, 0.10957105457782745, 0.10853202641010284]\n",
      "extrinsic_reward :  1 intrinsic_reward :  1.734386682510376\n",
      "place 42\n",
      "env number :  1 , global_step :  9 , action :  2 action_prob :  [0.16798150539398193, 0.16142818331718445, 0.17725905776023865, 0.11301307380199432, 0.1621902585029602, 0.109583280980587, 0.10854462534189224]\n",
      "extrinsic_reward :  0 intrinsic_reward :  1.3720752000808716\n",
      "place 42\n",
      "env number :  1 , global_step :  10 , action :  0 action_prob :  [0.16797393560409546, 0.1614227294921875, 0.1772478222846985, 0.11302237957715988, 0.16218478977680206, 0.10959334671497345, 0.10855498909950256]\n",
      "extrinsic_reward :  1 intrinsic_reward :  2.3914968967437744\n",
      "place 43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env number :  2 , global_step :  6 , action :  2 action_prob :  [0.16800275444984436, 0.16144351661205292, 0.1772906333208084, 0.1129869669675827, 0.16220560669898987, 0.10955499857664108, 0.10851547867059708]\n",
      "extrinsic_reward :  1 intrinsic_reward :  -0.44518229365348816\n",
      "place 41\n",
      "env number :  2 , global_step :  7 , action :  1 action_prob :  [0.16799890995025635, 0.16144074499607086, 0.17728491127490997, 0.11299169808626175, 0.16220282018184662, 0.10956013947725296, 0.10852076858282089]\n",
      "extrinsic_reward :  0 intrinsic_reward :  2.4492809772491455\n",
      "place 41\n",
      "env number :  2 , global_step :  8 , action :  6 action_prob :  [0.16799069941043854, 0.1614348143339157, 0.1772727072238922, 0.11300178617238998, 0.16219688951969147, 0.10957105457782745, 0.10853202641010284]\n",
      "extrinsic_reward :  1 intrinsic_reward :  1.734386682510376\n",
      "place 42\n",
      "env number :  2 , global_step :  9 , action :  4 action_prob :  [0.16798150539398193, 0.16142818331718445, 0.17725905776023865, 0.11301307380199432, 0.1621902585029602, 0.109583280980587, 0.10854462534189224]\n",
      "extrinsic_reward :  0 intrinsic_reward :  1.3720752000808716\n",
      "place 42\n",
      "env number :  2 , global_step :  10 , action :  0 action_prob :  [0.16797393560409546, 0.1614227294921875, 0.1772478222846985, 0.11302237957715988, 0.16218478977680206, 0.10959334671497345, 0.10855498909950256]\n",
      "extrinsic_reward :  1 intrinsic_reward :  2.3914968967437744\n",
      "place 43\n",
      "env number :  3 , global_step :  6 , action :  2 action_prob :  [0.16800126433372498, 0.16144244372844696, 0.17728839814662933, 0.11298883706331253, 0.1622045338153839, 0.10955704003572464, 0.108517587184906]\n",
      "extrinsic_reward :  0 intrinsic_reward :  -0.44293543696403503\n",
      "place 40\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-b57e73ea126c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     78\u001b[0m                                 \u001b[0maction_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextrinsic_reward_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m                                 \u001b[1;33m(\u001b[0m\u001b[0mintrinsic_reward\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m                                 action_prob_list[i][0][action_list[i]].item(), done_list[i])))\n\u001b[0m\u001b[0;32m     81\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'env number : '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m', global_step : '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mglobal_step_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m', action : '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'action_prob : '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maction_prob_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'extrinsic_reward : '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mextrinsic_reward_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'intrinsic_reward : '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mintrinsic_reward\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "T_horizon_list = [32,64,96,128]\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    clear_output()\n",
    "    global_step_list = [0 for _ in range(env_num)]\n",
    "    model.intrinsic_queue = [queue.Queue() for _ in range(env_num)]\n",
    "    model.intrinsic_input_queue = [queue.Queue() for _ in range(env_num)]\n",
    "    state_list = [env.reset() for env in env_list]\n",
    "    state_list = [np.array(state)/255 for state in state_list]\n",
    "            #state = np.transpose(state,(2,0,1))\n",
    "    state_list = [np.moveaxis(state, -1, 0) for state in state_list]\n",
    "    state_list = [torch.tensor(state).float() for state in state_list]\n",
    "    state_list = [state.unsqueeze(0) for state in state_list]\n",
    "    done_list = [False for _ in range(env_num)]\n",
    "    \n",
    "    action_prob_list = [[] for _ in range(env_num)]\n",
    "    m_list = [[] for _ in range(env_num)]\n",
    "    action_list = [[] for _ in range(env_num)]\n",
    "    next_state_list = [[] for _ in range(env_num)]\n",
    "    extrinsic_reward_list = [[] for _ in range(env_num)]\n",
    "    info_list = [[] for _ in range(env_num)]\n",
    "    next_state_list = [[] for _ in range(env_num)]\n",
    "    while not all(done_list) :\n",
    "        for i in range(env_num):\n",
    "            if not done_list[i] :\n",
    "                for t in range(T_horizon_list[i]):\n",
    "                    #env.render()\n",
    "                    global_step_list[i] +=1\n",
    "\n",
    "                    if gpu:\n",
    "                        action_prob_list[i], _ , _ = model.ppo.forward(state_list[i].cuda())\n",
    "                    else:\n",
    "                        action_prob_list[i], _ , _ = model.ppo.forward(state_list[i])\n",
    "                    m_list[i] = Categorical(action_prob_list[i])\n",
    "\n",
    "                    action_list[i] = m_list[i].sample().item()\n",
    "\n",
    "\n",
    "                    next_state, extrinsic_reward_list[i], done_list[i], info_list[i] = env_list[i].step(action_list[i])\n",
    "                    next_state = np.array(next_state)/255\n",
    "                    next_state = np.moveaxis(next_state,-1,0)\n",
    "                    next_state = torch.tensor(next_state).float()\n",
    "                    next_state = next_state.unsqueeze(0)\n",
    "                    next_state_list[i] = next_state\n",
    "\n",
    "                    model.intrinsic_input_queue_list[i].put(next_state)\n",
    "                    if len(model.intrinsic_input_queue_list[i].queue) > 128:\n",
    "                        model.intrinsic_input_queue_list[i].get()\n",
    "                    intrinsic_next_state_mean = \\\n",
    "                            torch.mean(torch.cat(list(model.intrinsic_input_queue_list[i].queue)),dim = 0)\n",
    "                    if len(model.intrinsic_input_queue_list[i].queue) == 1:\n",
    "                        intrinsic_next_state_std = torch.zeros(1)\n",
    "                    else:\n",
    "                        intrinsic_next_state_std = \\\n",
    "                                torch.std(torch.cat(list(model.intrinsic_input_queue_list[i].queue)),dim = 0)\n",
    "\n",
    "                    preprocessed_next_state = \\\n",
    "                            torch.clamp(((next_state - intrinsic_next_state_mean) / (intrinsic_next_state_std + 1e-8)), -5,5)\n",
    "\n",
    "\n",
    "                    #(model.intrinsic_input_queue)\n",
    "                    if gpu:\n",
    "                        predictor,target = model.rnd.forward(preprocessed_next_state.cuda())\n",
    "                    else:\n",
    "                        predictor,target = model.rnd.forward(preprocessed_next_state)\n",
    "                    intrinsic_reward = (predictor - target).pow(2).sum(1) / 2\n",
    "                    if len(model.intrinsic_queue_list[i].queue) > 128:\n",
    "                        model.intrinsic_queue_list[i].get()\n",
    "                    model.intrinsic_queue_list[i].put(intrinsic_reward.item())\n",
    "                    intrinsic_mean = np.mean(model.intrinsic_queue_list[i].queue)\n",
    "                    if len(model.intrinsic_queue_list[i].queue) == 1:\n",
    "                        if gpu:\n",
    "                            intrinsic_std = torch.zeros(1).cuda()\n",
    "                        else:\n",
    "                            intrinsic_std = torch.zeros(1)\n",
    "                    else:\n",
    "                        intrinsic_std = np.std(model.intrinsic_queue_list[i].queue)\n",
    "                    \n",
    "                    intrinsic_reward = (intrinsic_reward - intrinsic_mean) / (intrinsic_std+ 1e-8)\n",
    "\n",
    "                    if (info_list[i]['time'] == 0)  or(global_step_list[i] > 1000):\n",
    "                        done_list[i] = True\n",
    "                        reward = -10.\n",
    "\n",
    "                        \n",
    "                        \n",
    "                    model.put_data(i,((state_list[i].tolist(), \\\n",
    "                                    action_list[i], extrinsic_reward_list[i],\\\n",
    "                                    (intrinsic_reward.item()), next_state.tolist(), \\\n",
    "                                    action_prob_list[i][0][action_list[i]].item(), done_list[i])))\n",
    "                    print('env number : ',i,', global_step : ',global_step_list[i],', action : ', action_list[i],'action_prob : ',action_prob_list[i].tolist()[0])\n",
    "                    print('extrinsic_reward : ',extrinsic_reward_list[i],'intrinsic_reward : ',intrinsic_reward.item())\n",
    "                    print('place',info_list[i]['x_pos'])\n",
    "                    #print('info',info_list[i])\n",
    "                    if done_list[i] :\n",
    "                        print('epoch : ',epoch, ', global_step : ',global_step_list[i])\n",
    "                        break\n",
    "                    state_list[i] = next_state_list[i]\n",
    "        for i in range(env_num) :\n",
    "            if len(model.memory[i]) > 1:\n",
    "                model.train(i)\n",
    "    #env.render()\n",
    "\n",
    "#env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "env_num = 4\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "\n",
    "env_list = [gym_super_mario_bros.make('SuperMarioBros-v0') for _ in range(env_num)]\n",
    "env_list = [JoypadSpace(env, SIMPLE_MOVEMENT) for env in env_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_list = [gym_super_mario_bros.make('SuperMarioBros-1-'+str(i)+'-v0') for i in range(1,env_num+1)]\n",
    "env_list = [JoypadSpace(env, SIMPLE_MOVEMENT) for env in env_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_list[3].render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_list[3].close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "env =gym_super_mario_bros.make('SuperMarioBros-1-4-v0')\n",
    "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['NOOP'],\n",
       " ['right'],\n",
       " ['right', 'A'],\n",
       " ['right', 'B'],\n",
       " ['right', 'A', 'B'],\n",
       " ['A'],\n",
       " ['left']]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unity",
   "language": "python",
   "name": "unity"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
