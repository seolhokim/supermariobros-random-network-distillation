{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from torch.nn import init\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "import queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RND(nn.Module):\n",
    "    def __init__(self,width = 240, height =256, channel = 3):\n",
    "        super(RND,self).__init__()\n",
    "        \n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.channel = channel\n",
    "        \n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=3,\n",
    "                out_channels=32,\n",
    "                kernel_size=8,\n",
    "                stride=4),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(\n",
    "                in_channels=32,\n",
    "                out_channels=64,\n",
    "                kernel_size=4,\n",
    "                stride=2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(\n",
    "                in_channels=64,\n",
    "                out_channels=64,\n",
    "                kernel_size=3,\n",
    "                stride=1),\n",
    "            nn.LeakyReLU(),\n",
    "            Flatten(),\n",
    "            nn.Linear(46592, 512), # change\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512)\n",
    "            )\n",
    "        self.target = copy.deepcopy(self.predictor)\n",
    "        for p in self.modules():\n",
    "            if isinstance(p, nn.Conv2d):\n",
    "                init.orthogonal_(p.weight, np.sqrt(2))\n",
    "                p.bias.data.zero_()\n",
    "\n",
    "            if isinstance(p, nn.Linear):\n",
    "                init.orthogonal_(p.weight, np.sqrt(2))\n",
    "                p.bias.data.zero_()\n",
    "\n",
    "        for param in self.target.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "    def forward(self, state):\n",
    "        target_feature = self.target(state)\n",
    "        predict_feature = self.predictor(state)\n",
    "        return predict_feature, target_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "class PPO(nn.Module):\n",
    "    def __init__(self,width = 240, height =256, channel = 3, action_dim = 7):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.channel = channel\n",
    "        self.action_dim = action_dim\n",
    "        super(PPO,self).__init__()\n",
    "\n",
    "        \n",
    "        self.basic = nn.Sequential(\\\n",
    "                    nn.Conv2d(in_channels = 3,\\\n",
    "                             out_channels = 32,\\\n",
    "                             kernel_size = 8,\\\n",
    "                              stride = 4),\n",
    "                    nn.ReLU(),\n",
    "                                   nn.Conv2d(in_channels = 32,\\\n",
    "                                           out_channels = 64,\\\n",
    "                                           kernel_size = 4,\\\n",
    "                                           stride = 2),\\\n",
    "                    nn.ReLU(),\\\n",
    "                    nn.Conv2d(in_channels = 64,\\\n",
    "                             out_channels = 64,\\\n",
    "                             kernel_size = 3,\\\n",
    "                             stride = 1),\n",
    "                                   nn.ReLU(),\\\n",
    "                                   Flatten(),\n",
    "                    nn.Linear(46592,256), #have to change       \n",
    "                    nn.ReLU(),\\\n",
    "                    nn.Linear(256,448),\n",
    "                    nn.ReLU()\n",
    "                    )\n",
    "        self.actor = nn.Sequential(\\\n",
    "                                  nn.Linear(448,448),\\\n",
    "                                  nn.ReLU(),\\\n",
    "                                  nn.Linear(448,self.action_dim)\\\n",
    "                                  )\n",
    "        \n",
    "        self.extrinsic_critic = nn.Linear(448,1)\n",
    "        self.intrinsic_critic = nn.Linear(448,1)\n",
    "        \n",
    "        init.orthogonal_(self.extrinsic_critic.weight, 0.01)\n",
    "        self.extrinsic_critic.bias.data.zero_()\n",
    "\n",
    "        init.orthogonal_(self.intrinsic_critic.weight, 0.01)\n",
    "        self.intrinsic_critic.bias.data.zero_()\n",
    "\n",
    "        for i in range(len(self.actor)):\n",
    "            if type(self.actor[i]) == nn.Linear:\n",
    "                init.orthogonal_(self.actor[i].weight, 0.01)\n",
    "                self.actor[i].bias.data.zero_()\n",
    "    def forward(self, x,dim = -1):\n",
    "        x = self.basic(x)\n",
    "        action = self.actor(x)\n",
    "        action_prob = F.softmax(action,dim = dim)\n",
    "        \n",
    "        intrinsic = self.intrinsic_critic(x)\n",
    "        extrinsic = self.extrinsic_critic(x)\n",
    "        return action_prob,extrinsic,intrinsic    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "gamma         = 0.98\n",
    "lmbda         = 0.95\n",
    "eps_clip      = 0.1\n",
    "K_epoch       = 4\n",
    "T_horizon     = 128\n",
    "critic_coef = 0.5\n",
    "ent_coef = 0.001\n",
    "intrinsic_gamma = 0.99\n",
    "extrinsic_gamma = 0.999\n",
    "update_proportion = 0.25\n",
    "\n",
    "extrinsic_advantage_coef = 2\n",
    "intrinsic_advantage_coef = 1\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self,width=240,height=256,channel = 3,action_dim=7,learning_rate=0.0005):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.channel = channel\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        super(Agent,self).__init__()\n",
    "        \n",
    "        self.memory = [[] for _ in range(env_num)]\n",
    "        self.intrinsic_queue_list = [queue.Queue() for _ in range(env_num)]\n",
    "        self.intrinsic_input_queue_list = [queue.Queue() for _ in range(env_num)]\n",
    "        self.ppo = PPO(self.width, self.height, self.channel, self.action_dim)\n",
    "        self.rnd = RND(self.width, self.height , self.channel)\n",
    "        self.optimizer = optim.Adam(self.parameters(),lr = learning_rate)\n",
    "    def put_data(self,i,data):\n",
    "        self.memory[i].append(data)\n",
    "    def make_batch(self):\n",
    "        state_list, action_list, extrinsic_reward_list, intrinsic_reward_list, next_state_list, \\\n",
    "        prob_list, extrinsic_done_list,intrinsic_done_list = [],[],[],[],[],[], [],[]\n",
    "        for data in self.memory[i]:\n",
    "            state,action,extrinsic_reward, intrinsic_reward,next_state,prob,done = data\n",
    "            state_list.append(state)\n",
    "            action_list.append([action])\n",
    "            extrinsic_reward_list.append([extrinsic_reward])\n",
    "            intrinsic_reward_list.append([intrinsic_reward])\n",
    "\n",
    "            next_state_list.append(next_state)\n",
    "            extrinsic_done_mask = 0 if done else 1\n",
    "            extrinsic_done_list.append([extrinsic_done_mask])\n",
    "            intrinsic_done_list.append([1])\n",
    "            prob_list.append([prob])\n",
    "        self.memory[i] = []\n",
    "\n",
    "        s,a,er,ir,next_s,extrinsic_done_list,intrinsic_done_list,prob \\\n",
    "                                        = torch.tensor(state_list,dtype=torch.float),\\\n",
    "                                        torch.tensor(action_list),\\\n",
    "                                        torch.tensor(extrinsic_reward_list,dtype = torch.float),\\\n",
    "                                        torch.tensor(intrinsic_reward_list,dtype = torch.float),\\\n",
    "                                        torch.tensor(next_state_list,dtype=torch.float),\\\n",
    "                                        torch.tensor(extrinsic_done_list,dtype = torch.float),\\\n",
    "                                        torch.tensor(intrinsic_done_list,dtype = torch.float),\\\n",
    "                                        torch.tensor(prob_list,dtype = torch.float)\n",
    "        if gpu:\n",
    "            return s.cuda(),a.cuda(),er.cuda(),ir.cuda(),next_s.cuda(),extrinsic_done_list.cuda()\\\n",
    "            ,intrinsic_done_list.cuda(),prob.cuda() \n",
    "        else :\n",
    "            return s,a,er,ir,next_s,extrinsic_done_list,intrinsic_done_list,prob  \n",
    "    \n",
    "    def train(self,i):\n",
    "        state,action,extrinsic_reward,intrinsic_reward, next_state,extrinsic_done_list,\\\n",
    "        intrinsic_done_list,action_prob = self.make_batch(i)\n",
    "        \n",
    "        for i in range(K_epoch):\n",
    "            state = state.squeeze()\n",
    "            next_state = next_state.squeeze()\n",
    "            predicted_action, predicted_extrinsic, predicted_intrinsic = self.ppo(state)\n",
    "            predicted_next_action, predicted_next_extrinsic, predicted_next_intrinsic = self.ppo(next_state)\n",
    "\n",
    "            if gpu:\n",
    "                intrinsic_next_state_mean = torch.mean(torch.cat(list(self.intrinsic_input_queue.queue)),dim = 0).cuda()\n",
    "                intrinsic_next_state_std = torch.std(torch.cat(list(model.intrinsic_input_queue.queue)),dim = 0).cuda()\n",
    "            \n",
    "                preprocessed_next_state = torch.clamp(((next_state - intrinsic_next_state_mean) / \\\n",
    "                                                   intrinsic_next_state_std + torch.tensor(1e-8).cuda()),-5,5)\n",
    "            else:\n",
    "                intrinsic_next_state_mean = torch.mean(torch.cat(list(self.intrinsic_input_queue.queue)),dim = 0)\n",
    "                intrinsic_next_state_std = torch.std(torch.cat(list(model.intrinsic_input_queue.queue)),dim = 0)\n",
    "            \n",
    "                preprocessed_next_state = torch.clamp(((next_state - intrinsic_next_state_mean) / \\\n",
    "                                                   intrinsic_next_state_std + torch.tensor(1e-8)),-5,5)\n",
    "            \n",
    "            predict_feature, target_feature = self.rnd(next_state)\n",
    "            td_error = extrinsic_reward + extrinsic_gamma * predicted_next_extrinsic * extrinsic_done_list\n",
    "            delta = td_error - predicted_next_extrinsic\n",
    "            if gpu:\n",
    "                delta = delta.detach().cpu().numpy()\n",
    "            else:\n",
    "                delta = delta.detach().numpy()\n",
    "            advantage_list = []\n",
    "            \n",
    "            advantage = 0.0\n",
    "            for delta_t in delta[::-1]:\n",
    "                advantage = gamma * lmbda * advantage + delta_t[0]\n",
    "                advantage_list.append([advantage])\n",
    "            advantage_list.reverse()\n",
    "            if gpu:\n",
    "                advantage = torch.tensor(advantage_list,dtype = torch.float).cuda()\n",
    "            else:\n",
    "                advantage = torch.tensor(advantage_list,dtype = torch.float)\n",
    "            ##intrinsic_advantage\n",
    "            intrinsic_td_error = intrinsic_reward + intrinsic_gamma * predicted_next_intrinsic * intrinsic_done_list\n",
    "            intrinsic_delta = intrinsic_td_error - predicted_next_intrinsic\n",
    "            if gpu:\n",
    "                intrinsic_delta = intrinsic_delta.detach().cpu().numpy()\n",
    "            else:\n",
    "                intrinsic_delta = intrinsic_delta.detach().numpy()\n",
    "                \n",
    "            intrinsic_advantage_list = []\n",
    "            intrinsic_advantage = 0.0\n",
    "            \n",
    "            for intrinsic_delta_t in intrinsic_delta[::-1]:\n",
    "                intrinsic_advantage = gamma * lmbda * intrinsic_advantage + intrinsic_delta_t[0]\n",
    "                intrinsic_advantage_list.append([intrinsic_advantage])\n",
    "            intrinsic_advantage_list.reverse()\n",
    "            if gpu:\n",
    "                intrinsic_advantage = torch.tensor(intrinsic_advantage_list,dtype = torch.float).cuda()\n",
    "            else:\n",
    "                intrinsic_advantage = torch.tensor(intrinsic_advantage_list,dtype = torch.float)\n",
    "            #### intrinsic_error\n",
    "            intrinsic_error = (intrinsic_td_error - predicted_intrinsic.detach()).pow(2)\n",
    "            if gpu:\n",
    "                masking = torch.rand(len(intrinsic_error)).cuda()\n",
    "                masking = (masking < update_proportion).type(torch.FloatTensor).cuda()\n",
    "            else:\n",
    "                masking = torch.rand(len(intrinsic_error))\n",
    "                masking = (masking < update_proportion).type(torch.FloatTensor)\n",
    "            if gpu:\n",
    "                intrinsic_error = (intrinsic_error * masking).sum() / torch.max(intrinsic_error.sum(), torch.Tensor([1]).cuda())\n",
    "            else:\n",
    "                intrinsic_error = (intrinsic_error * masking).sum() / torch.max(masking.sum(), torch.Tensor([1]))\n",
    "\n",
    "            \n",
    "            now_action = predicted_action\n",
    "            m = Categorical(now_action)\n",
    "            entropy = m.entropy().mean()\n",
    "            \n",
    "            \n",
    "            now_action = now_action.gather(1,action)\n",
    "            \n",
    "            \n",
    "            ratio = torch.exp(torch.log(now_action) - torch.log(action_prob))\n",
    "            advantage = extrinsic_advantage_coef * advantage +  intrinsic_advantage_coef * intrinsic_advantage\n",
    "            surr1 = ratio * advantage\n",
    "            surr2 = torch.clamp(ratio , 1-eps_clip, 1 + eps_clip) * advantage\n",
    "            loss = - torch.min(surr1,surr2) + critic_coef * (F.smooth_l1_loss(predicted_extrinsic,td_error.detach()) +\\\n",
    "                    intrinsic_error) - ent_coef * entropy + F.mse_loss(predict_feature, target_feature)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "env_num = 4\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "\n",
    "env_list = [gym_super_mario_bros.make('SuperMarioBros-v0') for _ in range(env_num)]\n",
    "env_list = [JoypadSpace(env, SIMPLE_MOVEMENT) for env in env_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if gpu:\n",
    "    model = Agent().cuda()\n",
    "else:\n",
    "    model = Agent()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env number :  0 , global_step :  1 , action :  5 action_prob :  [0.14285747706890106, 0.14285731315612793, 0.14285649359226227, 0.142857164144516, 0.1428568810224533, 0.1428576111793518, 0.14285707473754883]\n",
      "extrinsic_reward :  0 intrinsic_reward :  0.7073347568511963\n",
      "place 40\n",
      "env number :  0 , global_step :  2 , action :  6 action_prob :  [0.14285747706890106, 0.14285731315612793, 0.14285649359226227, 0.142857164144516, 0.1428568810224533, 0.1428576111793518, 0.14285707473754883]\n",
      "extrinsic_reward :  0 intrinsic_reward :  0.6968941688537598\n",
      "place 40\n",
      "env number :  0 , global_step :  3 , action :  2 action_prob :  [0.14285747706890106, 0.14285731315612793, 0.14285649359226227, 0.142857164144516, 0.1428568810224533, 0.1428576111793518, 0.14285707473754883]\n",
      "extrinsic_reward :  -1 intrinsic_reward :  0.680672287940979\n",
      "place 39\n",
      "env number :  0 , global_step :  4 , action :  4 action_prob :  [0.14285746216773987, 0.14285729825496674, 0.14285649359226227, 0.142857164144516, 0.1428568810224533, 0.1428576111793518, 0.14285707473754883]\n",
      "extrinsic_reward :  0 intrinsic_reward :  0.6880443096160889\n",
      "place 39\n",
      "env number :  0 , global_step :  5 , action :  0 action_prob :  [0.14285747706890106, 0.14285731315612793, 0.14285649359226227, 0.142857164144516, 0.1428568959236145, 0.1428576111793518, 0.14285708963871002]\n",
      "extrinsic_reward :  1 intrinsic_reward :  0.6668408513069153\n",
      "place 40\n",
      "env number :  0 , global_step :  6 , action :  1 action_prob :  [0.14285746216773987, 0.14285731315612793, 0.14285649359226227, 0.142857164144516, 0.1428568810224533, 0.1428576111793518, 0.14285707473754883]\n",
      "extrinsic_reward :  0 intrinsic_reward :  0.6668626070022583\n",
      "place 40\n",
      "env number :  0 , global_step :  7 , action :  4 action_prob :  [0.14285746216773987, 0.14285731315612793, 0.14285650849342346, 0.142857164144516, 0.1428568810224533, 0.1428576111793518, 0.14285707473754883]\n",
      "extrinsic_reward :  0 intrinsic_reward :  0.42728132009506226\n",
      "place 40\n",
      "env number :  0 , global_step :  8 , action :  4 action_prob :  [0.14285747706890106, 0.14285731315612793, 0.14285650849342346, 0.142857164144516, 0.1428568959236145, 0.142857626080513, 0.14285708963871002]\n",
      "extrinsic_reward :  0 intrinsic_reward :  0.4122089743614197\n",
      "place 40\n",
      "env number :  0 , global_step :  9 , action :  3 action_prob :  [0.14285747706890106, 0.14285731315612793, 0.14285650849342346, 0.142857164144516, 0.1428568959236145, 0.142857626080513, 0.14285707473754883]\n",
      "extrinsic_reward :  0 intrinsic_reward :  0.4143984019756317\n",
      "place 40\n",
      "env number :  0 , global_step :  10 , action :  1 action_prob :  [0.14285747706890106, 0.14285731315612793, 0.14285650849342346, 0.142857164144516, 0.1428568959236145, 0.142857626080513, 0.14285707473754883]\n",
      "extrinsic_reward :  1 intrinsic_reward :  0.5121922492980957\n",
      "place 41\n",
      "env number :  0 , global_step :  11 , action :  2 action_prob :  [0.14285747706890106, 0.14285731315612793, 0.14285650849342346, 0.142857164144516, 0.1428568959236145, 0.142857626080513, 0.14285708963871002]\n",
      "extrinsic_reward :  0 intrinsic_reward :  0.5078297853469849\n",
      "place 41\n",
      "env number :  0 , global_step :  12 , action :  4 action_prob :  [0.14285747706890106, 0.14285731315612793, 0.14285650849342346, 0.142857164144516, 0.1428568959236145, 0.142857626080513, 0.14285707473754883]\n",
      "extrinsic_reward :  1 intrinsic_reward :  0.5384252071380615\n",
      "place 42\n",
      "env number :  0 , global_step :  13 , action :  5 action_prob :  [0.14285747706890106, 0.14285731315612793, 0.14285650849342346, 0.142857164144516, 0.1428568959236145, 0.142857626080513, 0.14285708963871002]\n",
      "extrinsic_reward :  0 intrinsic_reward :  0.5238214731216431\n",
      "place 42\n",
      "env number :  0 , global_step :  14 , action :  3 action_prob :  [0.14285747706890106, 0.14285731315612793, 0.14285650849342346, 0.142857164144516, 0.1428568959236145, 0.142857626080513, 0.14285708963871002]\n",
      "extrinsic_reward :  1 intrinsic_reward :  0.9387432932853699\n",
      "place 43\n",
      "env number :  0 , global_step :  15 , action :  4 action_prob :  [0.14285747706890106, 0.14285731315612793, 0.14285650849342346, 0.142857164144516, 0.1428568959236145, 0.142857626080513, 0.14285708963871002]\n",
      "extrinsic_reward :  0 intrinsic_reward :  0.9191038608551025\n",
      "place 43\n",
      "env number :  0 , global_step :  16 , action :  5 action_prob :  [0.14285746216773987, 0.14285729825496674, 0.14285649359226227, 0.142857164144516, 0.1428568810224533, 0.1428576111793518, 0.14285708963871002]\n",
      "extrinsic_reward :  1 intrinsic_reward :  0.907977819442749\n",
      "place 44\n",
      "env number :  0 , global_step :  17 , action :  5 action_prob :  [0.14285746216773987, 0.14285731315612793, 0.14285649359226227, 0.142857164144516, 0.1428568810224533, 0.1428576111793518, 0.14285707473754883]\n",
      "extrinsic_reward :  1 intrinsic_reward :  1.4040563106536865\n",
      "place 45\n",
      "env number :  0 , global_step :  18 , action :  3 action_prob :  [0.14285747706890106, 0.14285729825496674, 0.14285650849342346, 0.142857164144516, 0.1428568959236145, 0.1428576111793518, 0.14285708963871002]\n",
      "extrinsic_reward :  1 intrinsic_reward :  1.3797985315322876\n",
      "place 46\n",
      "env number :  0 , global_step :  19 , action :  4 action_prob :  [0.14285746216773987, 0.14285729825496674, 0.14285649359226227, 0.142857164144516, 0.1428568810224533, 0.1428576111793518, 0.14285707473754883]\n",
      "extrinsic_reward :  -1 intrinsic_reward :  1.344992756843567\n",
      "place 46\n",
      "env number :  0 , global_step :  20 , action :  4 action_prob :  [0.14285746216773987, 0.14285729825496674, 0.14285649359226227, 0.142857164144516, 0.1428568810224533, 0.1428576111793518, 0.14285707473754883]\n",
      "extrinsic_reward :  1 intrinsic_reward :  1.3429633378982544\n",
      "place 47\n",
      "env number :  1 , global_step :  1 , action :  0 action_prob :  [0.14285747706890106, 0.14285731315612793, 0.14285649359226227, 0.142857164144516, 0.1428568810224533, 0.1428576111793518, 0.14285707473754883]\n",
      "extrinsic_reward :  0 intrinsic_reward :  -0.30271661281585693\n",
      "place 40\n",
      "env number :  1 , global_step :  2 , action :  6 action_prob :  [0.14285747706890106, 0.14285731315612793, 0.14285649359226227, 0.142857164144516, 0.1428568810224533, 0.1428576111793518, 0.14285707473754883]\n",
      "extrinsic_reward :  0 intrinsic_reward :  0.7621455788612366\n",
      "place 40\n",
      "env number :  1 , global_step :  3 , action :  3 action_prob :  [0.14285747706890106, 0.14285731315612793, 0.14285649359226227, 0.142857164144516, 0.1428568810224533, 0.1428576111793518, 0.14285707473754883]\n",
      "extrinsic_reward :  -1 intrinsic_reward :  0.7483410835266113\n",
      "place 39\n",
      "env number :  1 , global_step :  4 , action :  6 action_prob :  [0.14285747706890106, 0.14285731315612793, 0.14285649359226227, 0.142857164144516, 0.1428568810224533, 0.1428576111793518, 0.14285707473754883]\n",
      "extrinsic_reward :  0 intrinsic_reward :  0.7303693294525146\n",
      "place 39\n",
      "env number :  1 , global_step :  5 , action :  5 action_prob :  [0.14285747706890106, 0.14285731315612793, 0.14285649359226227, 0.142857164144516, 0.1428568810224533, 0.1428576111793518, 0.14285707473754883]\n",
      "extrinsic_reward :  0 intrinsic_reward :  0.7277334332466125\n",
      "place 39\n",
      "env number :  1 , global_step :  6 , action :  4 action_prob :  [0.14285747706890106, 0.14285731315612793, 0.14285650849342346, 0.142857164144516, 0.1428568959236145, 0.142857626080513, 0.14285708963871002]\n",
      "extrinsic_reward :  0 intrinsic_reward :  0.7113271355628967\n",
      "place 39\n",
      "env number :  1 , global_step :  7 , action :  3 action_prob :  [0.14285747706890106, 0.14285731315612793, 0.14285650849342346, 0.142857164144516, 0.1428568959236145, 0.142857626080513, 0.14285708963871002]\n",
      "extrinsic_reward :  0 intrinsic_reward :  0.7107473015785217\n",
      "place 39\n",
      "env number :  1 , global_step :  8 , action :  4 action_prob :  [0.14285746216773987, 0.14285731315612793, 0.14285649359226227, 0.142857164144516, 0.1428568810224533, 0.1428576111793518, 0.14285707473754883]\n",
      "extrinsic_reward :  0 intrinsic_reward :  0.6977910995483398\n",
      "place 39\n"
     ]
    }
   ],
   "source": [
    "T_horizon = 20\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    global_step_list = [0 for _ in range(env_num)]\n",
    "    model.intrinsic_queue = [queue.Queue() for _ in range(env_num)]\n",
    "    model.intrinsic_input_queue = [queue.Queue() for _ in range(env_num)]\n",
    "    state_list = [env.reset() for env in env_list]\n",
    "    state_list = [np.array(state)/255 for state in state_list]\n",
    "            #state = np.transpose(state,(2,0,1))\n",
    "    state_list = [np.moveaxis(state, -1, 0) for state in state_list]\n",
    "    state_list = [torch.tensor(state).float() for state in state_list]\n",
    "    state_list = [state.unsqueeze(0) for state in state_list]\n",
    "    done_list = [False for _ in range(env_num)]\n",
    "    \n",
    "    action_prob_list = [[] for _ in range(env_num)]\n",
    "    m_list = [[] for _ in range(env_num)]\n",
    "    action_list = [[] for _ in range(env_num)]\n",
    "    next_state_list = [[] for _ in range(env_num)]\n",
    "    extrinsic_reward_list = [[] for _ in range(env_num)]\n",
    "    info_list = [[] for _ in range(env_num)]\n",
    "    next_state_list = [[] for _ in range(env_num)]\n",
    "    while not all(done_list) :\n",
    "        for i in range(env_num):\n",
    "            for t in range(T_horizon):\n",
    "                #env.render()\n",
    "                global_step_list[i] +=1\n",
    "\n",
    "                if gpu:\n",
    "                    action_prob_list[i], _ , _ = model.ppo.forward(state_list[i].cuda())\n",
    "                else:\n",
    "                    action_prob_list[i], _ , _ = model.ppo.forward(state_list[i])\n",
    "                m_list[i] = Categorical(action_prob_list[i])\n",
    "                \n",
    "                action_list[i] = m_list[i].sample().item()\n",
    "\n",
    "\n",
    "                next_state, extrinsic_reward_list[i], done_list[i], info_list[i] = env_list[i].step(action_list[i])\n",
    "                next_state = np.array(next_state)/255\n",
    "                next_state = np.moveaxis(next_state,-1,0)\n",
    "                next_state = torch.tensor(next_state).float()\n",
    "                next_state = next_state.unsqueeze(0)\n",
    "                next_state_list[i] = next_state\n",
    "\n",
    "                model.intrinsic_input_queue_list[i].put(next_state)\n",
    "                if len(model.intrinsic_input_queue_list[i].queue) > 128:\n",
    "                    model.intrinsic_input_queue_list[i].get()\n",
    "                intrinsic_next_state_mean = \\\n",
    "                        torch.mean(torch.cat(list(model.intrinsic_input_queue_list[i].queue)),dim = 0)\n",
    "                if len(model.intrinsic_input_queue_list[i].queue) == 1:\n",
    "                    intrinsic_next_state_std = torch.zeros(1)\n",
    "                else:\n",
    "                    intrinsic_next_state_std = \\\n",
    "                            torch.std(torch.cat(list(model.intrinsic_input_queue_list[i].queue)),dim = 0)\n",
    "\n",
    "                preprocessed_next_state = \\\n",
    "                        torch.clamp(((next_state - intrinsic_next_state_mean) / (intrinsic_next_state_std + 1e-8)), -5,5)\n",
    "\n",
    "\n",
    "                #(model.intrinsic_input_queue)\n",
    "                if gpu:\n",
    "                    predictor,target = model.rnd.forward(preprocessed_next_state.cuda())\n",
    "                else:\n",
    "                    predictor,target = model.rnd.forward(preprocessed_next_state)\n",
    "                intrinsic_reward = (predictor - target).pow(2).sum(1) / 2\n",
    "                if len(model.intrinsic_queue_list[i].queue) > 128:\n",
    "                    model.intrinsic_queue_list[i].get()\n",
    "                model.intrinsic_queue_list[i].put(intrinsic_reward.item())\n",
    "                intrinsic_mean = np.mean(model.intrinsic_queue_list[i].queue)\n",
    "                intrinsic_std = np.std(model.intrinsic_queue_list[i].queue)\n",
    "                intrinsic_reward = (intrinsic_reward - intrinsic_mean) / (intrinsic_std+ 1e-8)\n",
    "\n",
    "                if info_list[i]['time'] == 0 or info_list[i]['life'] == 1 or info_list[i]['time'] == 300:\n",
    "                    done_list[i] = True\n",
    "                    reward = -10.\n",
    "\n",
    "                model.put_data(i,((state_list[i].tolist(), \\\n",
    "                                action_list[i], extrinsic_reward_list[i],\\\n",
    "                                (intrinsic_reward.item()), next_state.tolist(), \\\n",
    "                                action_prob_list[i][0][action_list[i]].item(), done_list[i])))\n",
    "                print('env number : ',i,', global_step : ',global_step_list[i],', action : ', action_list[i],'action_prob : ',action_prob_list[i].tolist()[0])\n",
    "                print('extrinsic_reward : ',extrinsic_reward_list[i],'intrinsic_reward : ',intrinsic_reward.item())\n",
    "                print('place',info_list[i]['x_pos'])\n",
    "                #print('info',info_list[i])\n",
    "                if done_list[i] :\n",
    "                    print('epoch : ',epoch, ', global_step : ',global_step_list[i])\n",
    "                    break\n",
    "                state_list[i] = next_state_list[i]\n",
    "        for i in range(env_num) :\n",
    "            if len(model.memory[i]) > 1:\n",
    "                model.train(i)\n",
    "    #env.render()\n",
    "\n",
    "#env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unity",
   "language": "python",
   "name": "unity"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
