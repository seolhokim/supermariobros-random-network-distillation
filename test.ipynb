{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from torch.nn import init\n",
    "import copy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RND(nn.Module):\n",
    "    def __init__(self,width = 240, height =256, channel = 3):\n",
    "        super(RND,self).__init__()\n",
    "        \n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.channel = channel\n",
    "        \n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=3,\n",
    "                out_channels=32,\n",
    "                kernel_size=8,\n",
    "                stride=4),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(\n",
    "                in_channels=32,\n",
    "                out_channels=64,\n",
    "                kernel_size=4,\n",
    "                stride=2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(\n",
    "                in_channels=64,\n",
    "                out_channels=64,\n",
    "                kernel_size=3,\n",
    "                stride=1),\n",
    "            nn.LeakyReLU(),\n",
    "            Flatten(),\n",
    "            nn.Linear(128, 512), # change\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512)\n",
    "            )\n",
    "        self.target = copy.deepcopy(self.predictor)\n",
    "        for p in self.modules():\n",
    "            if isinstance(p, nn.Conv2d):\n",
    "                init.orthogonal_(p.weight, np.sqrt(2))\n",
    "                p.bias.data.zero_()\n",
    "\n",
    "            if isinstance(p, nn.Linear):\n",
    "                init.orthogonal_(p.weight, np.sqrt(2))\n",
    "                p.bias.data.zero_()\n",
    "\n",
    "        for param in self.target.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "    def forward(self, state):\n",
    "        target_feature = self.target(state)\n",
    "        predict_feature = self.predictor(state)\n",
    "        return predict_feature, target_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "class PPO(nn.Module):\n",
    "    def __init__(self,width = 240, height =256, channel = 3, action_dim = 7):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.channel = channel\n",
    "        self.action_dim = action_dim\n",
    "        super(PPO,self).__init__()\n",
    "        self.memory = []\n",
    "        \n",
    "        self.basic = nn.Sequential(\\\n",
    "                    nn.Conv2d(in_channels = 3,\\\n",
    "                             out_channels = 32,\\\n",
    "                             kernel_size = 8,\\\n",
    "                              stride = 4),\n",
    "                    nn.ReLU(),\n",
    "                                   nn.Conv2d(in_channels = 32,\\\n",
    "                                           out_channels = 64,\\\n",
    "                                           kernel_size = 4,\\\n",
    "                                           stride = 2),\\\n",
    "                    nn.ReLU(),\\\n",
    "                    nn.Conv2d(in_channels = 64,\\\n",
    "                             out_channels = 64,\\\n",
    "                             kernel_size = 3,\\\n",
    "                             stride = 1),\n",
    "                                   nn.ReLU(),\\\n",
    "                                   Flatten(),\n",
    "                    nn.Linear(46592,256), #have to change       \n",
    "                    nn.ReLU(),\\\n",
    "                    nn.Linear(256,448),\n",
    "                    nn.ReLU()\n",
    "                    )\n",
    "        self.actor = nn.Sequential(\\\n",
    "                                  nn.Linear(448,448),\\\n",
    "                                  nn.ReLU(),\\\n",
    "                                  nn.Linear(448,self.action_dim)\\\n",
    "                                  )\n",
    "        \n",
    "        self.extrinsic_critic = nn.Linear(448,1)\n",
    "        self.intrinsic_critic = nn.Linear(448,1)\n",
    "        \n",
    "        init.orthogonal_(self.extrinsic_critic.weight, 0.01)\n",
    "        self.extrinsic_critic.bias.data.zero_()\n",
    "\n",
    "        init.orthogonal_(self.intrinsic_critic.weight, 0.01)\n",
    "        self.intrinsic_critic.bias.data.zero_()\n",
    "\n",
    "        for i in range(len(self.actor)):\n",
    "            if type(self.actor[i]) == nn.Linear:\n",
    "                init.orthogonal_(self.actor[i].weight, 0.01)\n",
    "                self.actor[i].bias.data.zero_()\n",
    "    def forward(self, x,dim = -1):\n",
    "        x = self.basic(x)\n",
    "        action = self.actor(x)\n",
    "        action_prob = F.softmax(action,dim = dim)\n",
    "        \n",
    "        intrinsic = self.intrinsic_critic(x)\n",
    "        extrinsic = self.extrinsic_critic(x)\n",
    "        return action_prob,extrinsic,intrinsic    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "gamma         = 0.98\n",
    "lmbda         = 0.95\n",
    "eps_clip      = 0.1\n",
    "K_epoch       = 4\n",
    "T_horizon     = 128\n",
    "critic_coef = 0.5\n",
    "ent_coef = 0.001\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self,width=240,height=256,channel = 3,action_dim=7,learning_rate=0.0005):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.channel = channel\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        super(Agent,self).__init__()\n",
    "        \n",
    "        self.memory = []\n",
    "        \n",
    "        self.ppo = PPO(self.width, self.height, self.channel, self.action_dim)\n",
    "        self.rnd = RND(self.width, self.height , self.channel)\n",
    "    \n",
    "    def put_data(self,data):\n",
    "        self.memory.append(data)\n",
    "    def make_batch(self,episodic):\n",
    "        state_list, action_list, extrinsic_reward_list, intrinsic_reward_list, next_state_list, \\\n",
    "        prob_list, extrinsic_done_list,intrinsic_done_list = [],[],[],[],[],[], [],[]\n",
    "        for data in self.memory:\n",
    "            state,action,extrinsic_reward, intrinsic_reward,next_state,prob,done = data\n",
    "            state_list.append(state)\n",
    "            action_list.append([action])\n",
    "            extrinsic_reward_list.append([extrinsic_reward])\n",
    "            intrinsic_reward_list.append([intrinsic_reward])\n",
    "            prob_list.append([prob])\n",
    "            next_state_list.append(next_state)\n",
    "            extrinsic_done_mask = 0 if done else 1\n",
    "            extrinsic_done_mask = 1\n",
    "            extrinsic_done_list.append([done_mask])\n",
    "            intrinsic_done_list.append([1])\n",
    "        self.memory = []\n",
    "        \n",
    "        s,a,er,ir,next_s,extrinsic_done_list,intrinsic_done_list,prob \\\n",
    "                                        = torch.tensor(state_list,dtype=torch.float),\\\n",
    "                                        torch.tensor(action_list),\\\n",
    "                                        torch.tensor(extrinsic_reward_list),\\\n",
    "                                        torch.tensor(intrinsic_reward_list),\\\n",
    "                                        torch.tensor(next_state_list,dtype=torch.float),\\\n",
    "                                        torch.tensor(extrinsic_done_list,dtype = torch.float),\\\n",
    "                                        torch.tensor(intrinsic_done_list,dtype = torch.float),\\\n",
    "                                        torch.tensor(prob_list)\n",
    "        return s,a,er,ir,next_s,extrinsic_done_list,intrinsic_done_list,prob  \n",
    "    \n",
    "    def train(self):\n",
    "        state,action,extrinsic_reward,intrinsic_reward, next_state,extrinsic_done_list,\\\n",
    "        intrinsic_done_list,action_prob = self.make_batch()\n",
    "        \n",
    "        for i in range(K_epoch):\n",
    "            predicted_action, predicted_extrinsic, predicted_intrinsic = self.ppo(state)\n",
    "            predicted_next_action, predicted_next_extrinsic, predicted_next_intrinsic = self.ppo(next_state)\n",
    "            td_error = extrinsic_reward + gamma * predicted_next_extrinsic * extrinsic_done_list\n",
    "            delta = td_error - predicted_next_extrinsic\n",
    "            delta = delta.detach().numpy()\n",
    "            advantage_list = []\n",
    "            advantage = 0.0\n",
    "            for delta_t in delta[::-1]:\n",
    "                advantage = gamma * lmbda * advantage + delta_t[0]\n",
    "                advantage_list.append([advantage])\n",
    "            advantage_list.reverse()\n",
    "            advantage = torch.tensor(advantage_list,dtype = torch.float)\n",
    "            \n",
    "            intrinsic_td_error = intrinsic_reward + gamma * predicted_next_intrinsic * intrinsic_done_list\n",
    "            intrinsic_delta = intrinsic_td_error - predicted_next_intrinsic\n",
    "            intrinsic_delta = intrinsic_delta.detach().numpy()\n",
    "            intrinsic_advantage_list = []\n",
    "            intrinsic_advantage = 0.0\n",
    "            for intrinsic_delta_t in intrinsic_delta[::-1]:\n",
    "                intrinsic_advantage = gamma * lmbda * intrinsic_advantage + intrinsic_delta_t[0]\n",
    "                intrinsic_advantage_list.append([intrinsic_advantage])\n",
    "            intrinsic_advantage_list.reverse()\n",
    "            intrinsic_advantage_list = torch.tensor(intrinsic_advantage_list,dtype = torch.float)\n",
    "            \n",
    "            now_action = predicted_action\n",
    "            m = Categorical(now_action)\n",
    "            entropy = m.entropy().mean()\n",
    "            now_action = now_action.gather(1,action)\n",
    "            \n",
    "            ratio = torch.exp(torch.log(now_action) - torch.log(action_prob))\n",
    "            \n",
    "            surr1 = ratio * advantage\n",
    "            surr2 = torch.clamp(ratio , 1-eps_clip, 1 + eps_clip) * advantage\n",
    "            loss = - torch.min(surr1,surr2) + critic_coef * (F.smooth_l1_loss(predicted_extrinsic,td_error.detach()) +\\\n",
    "                    F.smooth_l1_loss(predicted_intrinsic,intrinsic_td_error.detach())) + ent_coef * entropy \\\n",
    "                    + F.mse_loss(predicted_next_extrinsic, predicted_next_intrinsic)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            self.optimizer.step()\n",
    "        self.ppo.memory = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
    "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "model = Agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ie-49\\AppData\\Local\\Continuum\\anaconda3\\envs\\unity\\lib\\site-packages\\gym_super_mario_bros\\smb_env.py:148: RuntimeWarning: overflow encountered in ubyte_scalars\n",
      "  return (self.ram[0x86] - self.ram[0x071c]) % 256\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    global_step = 0\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done :\n",
    "        for t in range(T_horizon):\n",
    "            env.render()\n",
    "            global_step +=1\n",
    "            state = np.array(state)/255\n",
    "            #state = np.transpose(state,(2,0,1))\n",
    "            state = np.moveaxis(state, -1, 0)\n",
    "            state = torch.tensor(state).float()\n",
    "            state = state.unsqueeze(0)\n",
    "            action_prob, _ , intrinsic_reward = model.ppo.forward(state,dim = 0)\n",
    "            m = Categorical(action_prob)\n",
    "            action = m.sample().item()\n",
    "                #state,action,extrinsic_reward,intrinsic_reward, next_state,done_mask,action_prob\n",
    "            action\n",
    "            next_state, extrinsic_reward, done, info = env.step(action)\n",
    "            if info['time'] == 0 :\n",
    "                done = True\n",
    "                reward = -10.\n",
    "            model.put_data((state, action, extrinsic_reward, intrinsic_reward, next_state, action_prob[0][action].item(), done))\n",
    "            #print('global_step : ',global_step,', action : ', action,' reward : ',reward, 'action_prob : ',action_prob)\n",
    "\n",
    "            if done :\n",
    "                print('epoch : ',epoch, ', global_step : ',global_step)\n",
    "                break\n",
    "            state = next_state\n",
    "    #env.render()\n",
    "\n",
    "#env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.8816e-07, -1.0887e-07, -5.4678e-07,  6.8075e-07,  2.4049e-06,\n",
       "           1.6383e-06,  3.7556e-06]], grad_fn=<AddmmBackward>),\n",
       " tensor([[1.3079e-06]], grad_fn=<AddmmBackward>),\n",
       " tensor([[-9.2827e-05]], grad_fn=<AddmmBackward>))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.ppo.forward(state,dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 4 is out of bounds for dimension 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-bd3a8064af90>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0maction_prob\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: index 4 is out of bounds for dimension 0 with size 1"
     ]
    }
   ],
   "source": [
    "action_prob[action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.4078, 0.4078, 0.4078,  ..., 0.4078, 0.4078, 0.4078],\n",
       "          [0.4078, 0.4078, 0.4078,  ..., 0.4078, 0.4078, 0.4078],\n",
       "          [0.4078, 0.4078, 0.4078,  ..., 0.4078, 0.4078, 0.4078],\n",
       "          ...,\n",
       "          [0.9412, 0.8941, 0.8941,  ..., 0.8941, 0.8941, 0.0000],\n",
       "          [0.9412, 0.8941, 0.8941,  ..., 0.8941, 0.0000, 0.0000],\n",
       "          [0.8941, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.8941]],\n",
       "\n",
       "         [[0.5333, 0.5333, 0.5333,  ..., 0.5333, 0.5333, 0.5333],\n",
       "          [0.5333, 0.5333, 0.5333,  ..., 0.5333, 0.5333, 0.5333],\n",
       "          [0.5333, 0.5333, 0.5333,  ..., 0.5333, 0.5333, 0.5333],\n",
       "          ...,\n",
       "          [0.8157, 0.3608, 0.3608,  ..., 0.3608, 0.3608, 0.0000],\n",
       "          [0.8157, 0.3608, 0.3608,  ..., 0.3608, 0.0000, 0.0000],\n",
       "          [0.3608, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.3608]],\n",
       "\n",
       "         [[0.9882, 0.9882, 0.9882,  ..., 0.9882, 0.9882, 0.9882],\n",
       "          [0.9882, 0.9882, 0.9882,  ..., 0.9882, 0.9882, 0.9882],\n",
       "          [0.9882, 0.9882, 0.9882,  ..., 0.9882, 0.9882, 0.9882],\n",
       "          ...,\n",
       "          [0.6902, 0.0627, 0.0627,  ..., 0.0627, 0.0627, 0.0000],\n",
       "          [0.6902, 0.0627, 0.0627,  ..., 0.0627, 0.0000, 0.0000],\n",
       "          [0.0627, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0627]]]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#Hyperparameters\n",
    "learning_rate = 0.0005\n",
    "gamma         = 0.98\n",
    "lmbda         = 0.95\n",
    "eps_clip      = 0.1\n",
    "K_epoch       = 3\n",
    "T_horizon     = 128\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self,width=240,height=256,channel = 3,action_dim=7,learning_rate=0.0005):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.channel = channel\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        super(Agent,self).__init__()\n",
    "        self.memory = []\n",
    "\n",
    "        self.conv_1 = nn.Conv2d(self.channel,16,3) #channel_1, kernel_1\n",
    "        self.pool_1 = nn.MaxPool2d(2) # channel_1\n",
    "        self.conv_2 = nn.Conv2d(16,16,3) # channel_1,channel_2, kernel_2\n",
    "        self.pool_2 = nn.MaxPool2d(2) # channel_2 \n",
    "        self.middle = nn.Linear(58*62*16,16)\n",
    "        self.policy = nn.Linear(16, self.action_dim) #바꿔야함\n",
    "        self.value = nn.Linear(16, 1) #바꿔야함\n",
    "        \n",
    "        self.rnd_\n",
    "        \n",
    "        \n",
    "        self.optimizer = optim.Adam(self.parameters(),lr = learning_rate)\n",
    "        \n",
    "    def get_action(self,x, softmax_dim = 0):\n",
    "        x = F.relu(self.conv_1(x))\n",
    "        x = self.pool_1(x)\n",
    "        x = F.relu(self.conv_2(x))\n",
    "        x = self.pool_2(x)\n",
    "        x = torch.flatten(x,start_dim=1)\n",
    "        x = self.middle(x)\n",
    "        x = self.policy(x)\n",
    "        if softmax_dim == 0 :\n",
    "            x = x.squeeze()\n",
    "        prob = F.softmax(x, dim = softmax_dim)\n",
    "        return prob\n",
    "    \n",
    "    def get_value(self,x):\n",
    "        x = F.relu(self.conv_1(x))\n",
    "        x = self.pool_1(x)\n",
    "        x = F.relu(self.conv_2(x))\n",
    "        x = self.pool_2(x)\n",
    "        x = torch.flatten(x,start_dim=1)\n",
    "        x = self.middle(x)\n",
    "        x = self.value(x)\n",
    "        return x\n",
    "    \n",
    "    def put_data(self,data):\n",
    "        self.memory.append(data)\n",
    "        \n",
    "    def make_batch(self):\n",
    "        state_list, action_list, reward_list, next_state_list, prob_list, done_list = [],[],[],[],[],[]\n",
    "        for data in self.memory:\n",
    "            state,action,reward,next_state,prob,done = data\n",
    "            state_list.append(state)\n",
    "            action_list.append([action])\n",
    "            reward_list.append([reward])\n",
    "            prob_list.append([prob])\n",
    "            next_state_list.append(next_state)\n",
    "            done_mask = 0 if done else 1\n",
    "            done_list.append([done_mask])\n",
    "        self.memory = []\n",
    "        \n",
    "        s,a,r,next_s,done_mask,prob = torch.tensor(state_list,dtype=torch.float),\\\n",
    "                                        torch.tensor(action_list),torch.tensor(reward_list),\\\n",
    "                                        torch.tensor(next_state_list,dtype=torch.float),\\\n",
    "                                        torch.tensor(done_list,dtype = torch.float),\\\n",
    "                                        torch.tensor(prob_list)\n",
    "        return s,a,r,next_s,done_mask,prob\n",
    "    \n",
    "    def train(self):\n",
    "        state,action,extrinsic_reward,intrinsic_reward, next_state,done_mask,action_prob = self.make_batch()\n",
    "\n",
    "        for i in range(K_epoch):\n",
    "            td_error = reward + gamma * self.get_value(next_state) * done_mask\n",
    "            delta = td_error - self.get_value(state)\n",
    "            delta = delta.detach().numpy()\n",
    "            advantage_list = []\n",
    "            advantage = 0.0\n",
    "            for delta_t in delta[::-1]:\n",
    "                advantage = gamma * lmbda * advantage + delta_t[0]\n",
    "                advantage_list.append([advantage])\n",
    "            advantage_list.reverse()\n",
    "            advantage = torch.tensor(advantage_list,dtype = torch.float)\n",
    "            \n",
    "            \n",
    "            now_action = self.get_action(state,softmax_dim = 1)\n",
    "            now_action = now_action.gather(1,action)\n",
    "            \n",
    "            ratio = torch.exp(torch.log(now_action) - torch.log(action_prob))\n",
    "            \n",
    "            surr1 = ratio * advantage\n",
    "            surr2 = torch.clamp(ratio , 1-eps_clip, 1 + eps_clip) * advantage\n",
    "            loss = - torch.min(surr1,surr2) + F.smooth_l1_loss(self.get_value(state),td_error.detach())\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            self.optimizer.step()\n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1., 1.]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1., 1.]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240, 256, 3)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[104., 104., 104.,  ..., 104., 104., 104.],\n",
       "          [104., 104., 104.,  ..., 104., 104., 104.],\n",
       "          [104., 104., 104.,  ..., 104., 104., 104.],\n",
       "          ...,\n",
       "          [228., 228., 228.,  ..., 228., 228., 228.],\n",
       "          [228., 228.,   0.,  ..., 228., 228., 228.],\n",
       "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.]],\n",
       "\n",
       "         [[136., 136., 136.,  ..., 136., 136., 136.],\n",
       "          [136., 136., 136.,  ..., 136., 136., 136.],\n",
       "          [136., 136., 136.,  ..., 136., 136., 136.],\n",
       "          ...,\n",
       "          [ 92.,  92.,  92.,  ...,  92.,  92.,  92.],\n",
       "          [ 92.,  92.,   0.,  ...,  92.,  92.,  92.],\n",
       "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.]],\n",
       "\n",
       "         [[252., 252., 252.,  ..., 252., 252., 252.],\n",
       "          [252., 252., 252.,  ..., 252., 252., 252.],\n",
       "          [252., 252., 252.,  ..., 252., 252., 252.],\n",
       "          ...,\n",
       "          [ 16.,  16.,  16.,  ...,  16.,  16.,  16.],\n",
       "          [ 16.,  16.,   0.,  ...,  16.,  16.,  16.],\n",
       "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.]]]])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['NOOP'],\n",
       " ['right'],\n",
       " ['right', 'A'],\n",
       " ['right', 'B'],\n",
       " ['right', 'A', 'B'],\n",
       " ['A'],\n",
       " ['left']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SIMPLE_MOVEMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "some of the strides of a given numpy array are negative. This is currently not supported, but will be added in future releases.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-fe7162c08038>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: some of the strides of a given numpy array are negative. This is currently not supported, but will be added in future releases."
     ]
    }
   ],
   "source": [
    "torch.tensor(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "state =np.array(state)\n",
    "#test_state = torch.tensor(np.array(state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240, 256, 3)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_state = np.transpose(state,(2,0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_state = test_state.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_state = torch.tensor(test_state)\n",
    "test_state = test_state.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.7245e+02, -1.7245e+02, -1.7245e+02,  ..., -1.7245e+02,\n",
       "           -1.7245e+02, -1.7245e+02],\n",
       "          [-1.7245e+02, -1.7245e+02, -1.7245e+02,  ..., -1.7245e+02,\n",
       "           -1.7245e+02, -1.7245e+02],\n",
       "          [-1.7245e+02, -1.7245e+02, -1.7245e+02,  ..., -1.7245e+02,\n",
       "           -1.7245e+02, -1.7245e+02],\n",
       "          ...,\n",
       "          [-5.5279e+01, -1.3196e+01, -3.9281e+01,  ...,  1.4265e+01,\n",
       "           -1.2687e+02, -9.1684e+01],\n",
       "          [-8.2662e+01, -9.2342e+01, -7.8128e+01,  ...,  4.2180e+00,\n",
       "           -6.6506e+01, -7.0498e+01],\n",
       "          [-3.9383e+01, -1.2426e+02, -8.1975e+01,  ...,  2.6676e-01,\n",
       "           -6.1623e+01, -7.2660e+00]],\n",
       "\n",
       "         [[-1.9878e+01, -1.9878e+01, -1.9878e+01,  ..., -1.9878e+01,\n",
       "           -1.9878e+01, -1.9878e+01],\n",
       "          [-1.9878e+01, -1.9878e+01, -1.9878e+01,  ..., -1.9878e+01,\n",
       "           -1.9878e+01, -1.9878e+01],\n",
       "          [-1.9878e+01, -1.9878e+01, -1.9878e+01,  ..., -1.9878e+01,\n",
       "           -1.9878e+01, -1.9878e+01],\n",
       "          ...,\n",
       "          [ 1.1519e+02,  9.6158e+01, -2.2805e+01,  ...,  8.6479e+01,\n",
       "            1.4433e+01,  8.0143e+01],\n",
       "          [ 7.7587e+01,  1.4945e+02, -6.6438e+01,  ...,  1.0818e+02,\n",
       "            5.4639e+01,  6.2714e+01],\n",
       "          [ 3.4523e+00,  9.1733e+01, -3.2363e+01,  ...,  5.1383e+01,\n",
       "            3.0275e+01,  9.0795e+01]],\n",
       "\n",
       "         [[ 8.8190e+01,  8.8190e+01,  8.8190e+01,  ...,  8.8190e+01,\n",
       "            8.8190e+01,  8.8190e+01],\n",
       "          [ 8.8190e+01,  8.8190e+01,  8.8190e+01,  ...,  8.8190e+01,\n",
       "            8.8190e+01,  8.8190e+01],\n",
       "          [ 8.8190e+01,  8.8190e+01,  8.8190e+01,  ...,  8.8190e+01,\n",
       "            8.8190e+01,  8.8190e+01],\n",
       "          ...,\n",
       "          [-1.1942e+02, -1.0600e+02, -7.4056e+01,  ...,  4.7143e+01,\n",
       "            3.9230e+01,  6.0196e+01],\n",
       "          [ 9.7123e+01,  7.6514e+01,  5.4489e+01,  ...,  1.6658e+01,\n",
       "            4.9782e+01,  2.3267e+01],\n",
       "          [ 6.9341e+01,  1.5261e+01,  6.4930e+01,  ...,  4.9522e+00,\n",
       "            3.1669e+01,  3.3062e+01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 5.3623e+01,  5.3623e+01,  5.3623e+01,  ...,  5.3623e+01,\n",
       "            5.3623e+01,  5.3623e+01],\n",
       "          [ 5.3623e+01,  5.3623e+01,  5.3623e+01,  ...,  5.3623e+01,\n",
       "            5.3623e+01,  5.3623e+01],\n",
       "          [ 5.3623e+01,  5.3623e+01,  5.3623e+01,  ...,  5.3623e+01,\n",
       "            5.3623e+01,  5.3623e+01],\n",
       "          ...,\n",
       "          [ 5.3734e+01,  5.4188e+01,  3.3643e+01,  ..., -2.6488e+01,\n",
       "           -7.1530e+01, -8.7496e+00],\n",
       "          [-1.0126e+02, -1.3275e+02, -5.8020e+01,  ..., -1.1688e+01,\n",
       "           -1.1031e+02, -8.4262e+01],\n",
       "          [-3.8969e+01, -9.1333e+01,  1.2491e+02,  ..., -5.1848e+01,\n",
       "           -9.9524e+01, -3.0445e+01]],\n",
       "\n",
       "         [[ 1.0568e+02,  1.0568e+02,  1.0568e+02,  ...,  1.0568e+02,\n",
       "            1.0568e+02,  1.0568e+02],\n",
       "          [ 1.0568e+02,  1.0568e+02,  1.0568e+02,  ...,  1.0568e+02,\n",
       "            1.0568e+02,  1.0568e+02],\n",
       "          [ 1.0568e+02,  1.0568e+02,  1.0568e+02,  ...,  1.0568e+02,\n",
       "            1.0568e+02,  1.0568e+02],\n",
       "          ...,\n",
       "          [ 1.3542e+02,  9.0664e+01,  1.3234e+02,  ...,  8.8940e+01,\n",
       "            1.7761e+02,  1.3905e+02],\n",
       "          [ 1.8117e+02,  1.1223e+02,  7.0980e+01,  ...,  9.8525e+01,\n",
       "            1.1443e+02,  7.9186e+01],\n",
       "          [-5.4967e+01,  1.3543e+01,  4.8670e+01,  ...,  8.3944e+01,\n",
       "           -8.0072e+00, -1.9819e+01]],\n",
       "\n",
       "         [[ 2.2161e+01,  2.2161e+01,  2.2161e+01,  ...,  2.2161e+01,\n",
       "            2.2161e+01,  2.2161e+01],\n",
       "          [ 2.2161e+01,  2.2161e+01,  2.2161e+01,  ...,  2.2161e+01,\n",
       "            2.2161e+01,  2.2161e+01],\n",
       "          [ 2.2161e+01,  2.2161e+01,  2.2161e+01,  ...,  2.2161e+01,\n",
       "            2.2161e+01,  2.2161e+01],\n",
       "          ...,\n",
       "          [-5.7928e+01, -1.4160e+01,  6.1129e+01,  ...,  1.3647e+01,\n",
       "            3.5898e+01, -4.2134e+01],\n",
       "          [-5.4619e+01, -1.0006e+02, -8.7376e+01,  ...,  8.1330e-02,\n",
       "           -3.1692e+01, -1.5534e+01],\n",
       "          [ 1.1032e+01,  6.8063e+00, -9.7168e+01,  ...,  3.1800e+00,\n",
       "           -5.0022e+01, -1.9700e+01]]]], grad_fn=<ThnnConv2DBackward>)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_conv(test_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_conv = nn.Conv2d(3,64,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([240, 256, 3])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 240, 256, 3])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_state.reshape(-1,240,256,3).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([240, 256, 3])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 256, 240])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_state.transpose(0,2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[104, 136, 252,  ..., 136, 252, 104],\n",
       "          [136, 252, 104,  ..., 252, 104, 136],\n",
       "          [252, 104, 136,  ..., 104, 136, 252],\n",
       "          ...,\n",
       "          [104, 136, 252,  ..., 136, 252, 104],\n",
       "          [136, 252, 104,  ..., 252, 104, 136],\n",
       "          [252, 104, 136,  ..., 104, 136, 252]],\n",
       "\n",
       "         [[104, 136, 252,  ..., 136, 252, 104],\n",
       "          [136, 252, 104,  ..., 252, 104, 136],\n",
       "          [252, 104, 136,  ..., 104, 136, 252],\n",
       "          ...,\n",
       "          [104, 136, 252,  ..., 136, 252, 104],\n",
       "          [136, 252, 104,  ..., 252, 104, 136],\n",
       "          [252, 104, 136,  ...,   0,   0,   0]],\n",
       "\n",
       "         [[104, 136, 252,  ..., 136, 252, 104],\n",
       "          [136, 252, 104,  ..., 252, 104, 136],\n",
       "          [252, 104, 136,  ..., 104, 136, 252],\n",
       "          ...,\n",
       "          [  0,   0,   0,  ..., 208, 176,   0],\n",
       "          [  0,   0,   0,  ...,   0,   0,   0],\n",
       "          [  0, 228,  92,  ...,   0,   0,   0]]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_state.reshape(-1,3,240,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 256, 240])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_state.permute(2,1,0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_conv(test_state.reshape(-1,240,256,3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unity",
   "language": "python",
   "name": "unity"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
